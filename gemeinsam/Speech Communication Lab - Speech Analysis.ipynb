{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Load Modules and Import Audio\n",
    "\n",
    "### Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# uncomment the following 2 lines to reload the modules automatically,\n",
    "# such that changes to Plot_functions.py are reloaded without restarting the kernel!\n",
    "#\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#\n",
    "###########################################################################################\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "import soundfile as sf\n",
    "import bokeh\n",
    "import sounddevice as sd\n",
    "import time as clock\n",
    "import Plot_functions as SCplot # imports the necessary plot functions\n",
    "import Helper_functions as SCtools # imports the necessary helper functions\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import signal, fft, ifft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Playback Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Choose an Audio File\n",
    "fileName = 'f116.wav'\n",
    "#fileName = 'f216.wav'\n",
    "#fileName = 'a_8000.wav'\n",
    "#fileName = '1000hz_3sec.wav'\n",
    "\n",
    "snd, audio1, fs, filePath = SCtools.import_sound_file(fileName)\n",
    "ipd.Audio(filePath) # show audio player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Time-Domain Analysis\n",
    "## Short-Time Average Energy (Intensity)\n",
    "\n",
    "In this experiment we will discuss the intensity curve in context with speech analysis and investigate the effects of different window lengths. To calculate the intensity curve, we firstly use our own function based on pythons 'signal' module. After that, we will move on to the methods provided by the library 'praat-parselmouth', whith provides the functionality of the software Praat in Python using the original Praat functions, which are written in C++.\n",
    "> Praat documentation: <https://www.fon.hum.uva.nl/praat/manual/Intro.html>\n",
    "\n",
    "\n",
    "### Own Implementation using pythons 'signal' library\n",
    "To calculate the short-time average intensity, we implement the function SC_intensity(). In this function, the signal gets averaged using a Gaussian window. A Gaussian window need 2 parameters, see [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.gaussian.html#scipy.signal.windows.gaussian), which are the window length in samples and the standard deviation. Unfortunately, Praat's documentation is not very helpful here, as it provides no information about the standard deviation. Therefore, we go over to [Matlab's documentation of the Gaussian window](https://de.mathworks.com/help/signal/ref/gausswin.html) where we seek help to find a value for the standard deviation, for which we trust Matlab's default value of alpha=2.5.\n",
    "\n",
    "### Task 1.1: Find a suitable value for the standard deviation with the help of Matlab's documentation of the Gaussian window.\n",
    "#### Expected Answer:\n",
    "- the student should recognize from Matlabs documentation that a reasonable value for the standard deviation (or std in the code below) is given as sigma = (L-1)/(2*alpha) with alpha=2.5, where L is the length of the window in samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "windowLength = 20 #millisecond\n",
    "\n",
    "#Calculate minimum pitch from window length\n",
    "minimumPitch = 1000/windowLength\n",
    "\n",
    "def SC_intensity(sound, minimumPitch, fs):\n",
    "    #winLenEffective = np.round(3.2/minimumPitch * fs)  # Window length in samples; from Praat documentation; for pitch-synchronous intensity ripple\n",
    "    winLenEffective = np.round(1/minimumPitch * fs)\n",
    "    \n",
    "    # To Do: find a suitable value for the standard deviation std\n",
    "    # std = ?\n",
    "    alpha = 2.5  # default value from Matlab documentation\n",
    "    std = (winLenEffective-1)/(2*alpha)  # standard deviation from Matlab documentation\n",
    "    win = signal.windows.gaussian(winLenEffective, std) # Gaussian window\n",
    "    #win = signal.windows.kaiser(winLenEffective, 20/np.pi) # Kaiser window - alternative to Gaussian window\n",
    "    \n",
    "    sound = np.square(sound-np.mean(sound)) \n",
    "\n",
    "    intensity = np.convolve(sound, win, mode='valid') /np.sum(win)\n",
    "    \n",
    "    intensity = 10*np.log10(intensity/(4e-10)) # conversion to dB ; norm to (20 muPa)^2\n",
    "\n",
    "    print(\"SC_intensity: Intensity has {} samples\".format(intensity.size))\n",
    "    winLenEffectiveTime = winLenEffective / fs\n",
    "    return intensity, win, winLenEffectiveTime\n",
    "\n",
    "SC_intensity, gaussWin, winLenEffectiveTime = SC_intensity(np.squeeze(snd.values), minimumPitch, fs)\n",
    "\n",
    "# Plot window function\n",
    "plottitle = \"Gaussian Window for Averaging with SC_intensity()\"\n",
    "dt_win = np.arange(0, gaussWin.size) / fs  # time axis for Gaussian Window\n",
    "p_window = SCplot.get_plot_window(gaussWin, dt_win, plottitle)\n",
    "\n",
    "# Plot intensity curve\n",
    "snd_values = np.squeeze(snd.values)\n",
    "dt_snd = np.arange(0, snd_values.size) / fs\n",
    "dt_SC_intensity = np.arange(0,SC_intensity.size) / fs \n",
    "dt_SC_intensity = dt_SC_intensity + winLenEffectiveTime/2 # shift to center the intensity curve bins in the windows\n",
    "\n",
    "plottitle = 'Intensity calculated with SC_intensity() - File: ' + fileName\n",
    "p_intensity = SCplot.get_plot_intensity(snd_values, dt_snd, SC_intensity, dt_SC_intensity, plottitle)\n",
    "\n",
    "SCplot.plot_in_subplots(p_window, p_intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Praat-Parselmouth implementation\n",
    "Also, the library praat-parselmouth provides us with functionality to calculate the intensity. To calculate the intensity with parselmouth, the member function to_intensity() is used, which takes the minimum pitch as an input argument. To compare parselmouth's to_intensity() with our custom SC_intensity(), we use the same input parameters as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PM_intensity = snd.to_intensity(minimum_pitch=minimumPitch, subtract_mean=False)  # intensity calculation with parselmouth's function\n",
    "PM_intensity = snd.to_intensity(minimum_pitch=minimumPitch, subtract_mean=False, time_step=1/fs)  # intensity calculation with parselmouth's function\n",
    "print(\"PM_intensity: Intensity has {} samples\".format(PM_intensity.get_number_of_frames()))\n",
    "\n",
    "dt_PM_intensity = PM_intensity.x_grid()[:-1]\n",
    "PM_intensity_val = np.squeeze(PM_intensity.values)\n",
    "dt_snd = snd.x_grid()[:-1]\n",
    "\n",
    "plottitle = \"Intensity calculated with parselmouth's to_intensity() - File: \" + fileName\n",
    "SCplot.get_plot_intensity(np.squeeze(snd.values), dt_snd, PM_intensity_val, dt_PM_intensity, plottitle, showPlot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the two intensity curves\n",
    "Compare the two intensity curves in the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 2 intensity curves in one plot\n",
    "plottitle = \"Comparison of both intensity curves - File: \" + fileName\n",
    "SCplot.plot_two_intensity_curves(dt_PM_intensity, PM_intensity_val, dt_SC_intensity, SC_intensity, plottitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Describe the key differences of the intensity curve calculated with your own function SC_intensity() to parselmouth's to_intensity().\n",
    "#### Expected Answers:\n",
    "- different value range\n",
    "- parselmouths intensity is more sparsely sampled\n",
    "- parselmouths intensity has not the same time range as the audio file, whereas the custom intensity covers the whole time range of the audio file\n",
    "\n",
    "### Task 1.3: What happens to the intensity curve, if we use different window sizes?\n",
    "#### Expected answers:\n",
    "- short windows allow more fluctuations of the intensity curve\n",
    "- long windows lead to more smoothing of the intensity curve, thus the fluctuations of the intensity curve will become smaller\n",
    "\n",
    "\n",
    "### Task 1.4: Is there a way to alter the intensity curve calculated with your own function SC_intensity(), such that it matches the intensity curve calculated with parselmouth better?\n",
    "> Hint: see <https://www.fon.hum.uva.nl/praat/manual/Sound__To_Intensity___.html>\n",
    "#### Expected Answer:\n",
    "- the student should recognize that parselmouth uses an effective window length that is 3.2 times the original window length\n",
    "- the student should modify the function SC_intensity() such that it uses this effective window length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Frequency-Domain Analysis\n",
    "In this experiment, we analyze the sound file in frequency domain, for example using spectrograms. Again, we implement a spectrogram using Python's 'signal' model, and compare it with Praat-Parselmouth's spectrogram.\n",
    "### Load and Playback Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an Audio File\n",
    "#fileName = 'f116.wav'\n",
    "#fileName = 'f216.wav'\n",
    "fileName = 'a_8000.wav'\n",
    "#fileName = '1000hz_3sec.wav'\n",
    "\n",
    "snd, audio1, fs, filePath = SCtools.import_sound_file(fileName)\n",
    "ipd.Audio(filePath) # show audio player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own implementation using Python's 'signal' module\n",
    "First, we start by calculating a spectrogram using the method scipy.signal.spectrogram()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "windowlengthSec = 30 #ms\n",
    "windowlength = np.round(fs * windowlengthSec/1000).astype(int)\n",
    "#windowlength = 2048\n",
    "print('Window Length in samples:', windowlength)\n",
    "#overlap = windowlength-1\n",
    "overlap = np.round(windowlength / 2)\n",
    "\n",
    "# Gaussian window\n",
    "std = (windowlength - 1)/(2*2.5)  # see Task 1.1\n",
    "window = ('gaussian', std) # tuple for signal.spectrogram\n",
    "\n",
    "#window = 'hann' # Hann window - alternative to Gaussian window\n",
    "\n",
    "SC_fVec, SC_tVec, SC_spectroData = signal.spectrogram(audio1, fs=fs, window=window, noverlap=overlap, nperseg=windowlength, return_onesided=True, scaling='spectrum', mode='magnitude')\n",
    "\n",
    "SC_spectroDataDB = 20*np.log10(SC_spectroData / np.max(SC_spectroData))\n",
    "\n",
    "# plot interactive spectrogram\n",
    "plottitle = \"Custom Spectrogram of Sound Sample - File: \" + fileName\n",
    "SC_timeWidget = SCplot.plot_interactive_spectrogram(SC_spectroDataDB, SC_tVec, SC_fVec, plottitle)\n",
    "widgets.HBox([SC_timeWidget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Praat-Parselmouth implementation\n",
    "Now we use parselmouths to_spectrogram() to calculate a spectrogram. For Plotting the spectrogram, we use our custum function plot_interactive_spectrogram()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "windowlengthSec = 70 #ms\n",
    "maximumFrequency = 5000\n",
    "\n",
    "def PM_get_spectrogram(snd, windowLengthMS=30, maximumFrequency=5000):\n",
    "    spectrogram = snd.to_spectrogram(window_length=windowLengthMS/1000, maximum_frequency=maximumFrequency)\n",
    "    PM_spectroData = spectrogram.values\n",
    "    PM_tVec = spectrogram.ts()\n",
    "    PM_fVec = spectrogram.ys()\n",
    "    PM_spectroDataDB = 10*np.log10(PM_spectroData / np.max(PM_spectroData))\n",
    "    return PM_spectroDataDB, PM_tVec, PM_fVec\n",
    "\n",
    "PM_spectroDataDB, PM_tVec, PM_fVec = PM_get_spectrogram(snd, windowLengthMS=windowlengthSec, maximumFrequency=maximumFrequency)\n",
    "\n",
    "plottitle = \"Parselmouth Spectrogram of Sound Sample - File: \" + fileName\n",
    "PM_timeWidget = SCplot.plot_interactive_spectrogram(PM_spectroDataDB, PM_tVec, PM_fVec, plottitle)\n",
    "widgets.HBox([PM_timeWidget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Effects of different window lengths\n",
    "Modify the window length in the spectrogram generated using Praat-Parselmouth and set it to the following values:\n",
    "- 10 ms\n",
    "- 30 ms\n",
    "- 70 ms\n",
    "\n",
    "Describe the effects you see in the spectrogram and in the spectrum. What could 'wide band analysis' and 'narrow band analysis' mean in this context?\n",
    "#### Expected answers:\n",
    "- the shorter the window, the better is the time resolution, but the frequencfy resolution gets worse\n",
    "- if we use long windows, the frequency resolution gets better, but the time resolution gets worse\n",
    "- 'wide band analysis' refers to spectrogram analysis using short windows, resulting in a good time resolution, but a poor frequency resolution (i.e. the frequencies are only displayed as 'wide bands')\n",
    "- 'narrow band analysis' refers to spectrogram analysis using long windows, thus with a good frequency resolution, but a poor time resolution\n",
    "\n",
    "### Task 2.2: Interpretation of the spectrum and the spectrogram\n",
    "What information can be retrieved from the spectrogram? Think of parameters of the signal that are easily represented in frequancy domain.\n",
    "#### Expected answers:\n",
    "- fundamental frequency f0\n",
    "- harmonic structure of the signal, formant structure\n",
    "- what kind of voice signal is it? (vowel, consonant or fricative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of f0 and formants\n",
    "To analyze the Formants, we use praat-parselmouths formant analysis methods. We analyse the fundamental frequency f0 and additionally the first four formants F1,...,F4.\n",
    "\n",
    "Select sound file to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an Audio File\n",
    "#fileName = 'f116.wav'\n",
    "#fileName = 'f216.wav'\n",
    "fileName = 'e_11025.wav'\n",
    "#fileName = 'a_8000.wav'\n",
    "#fileName = '1000hz_3sec.wav'\n",
    "\n",
    "snd, _, fs, filePath = SCtools.import_sound_file(fileName)\n",
    "ipd.Audio(filePath) # show audio player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the fundamental frequency and the formants, we use the Praat-parselmouth methods to_pitch() for the fundamental frequency and to_formant_burg() for the formants. For the formant analysis, the BURG-algorithm is used (more information about this algorithm, which is based on LPC internally, is [here](https://asa.scitation.org/doi/10.1121/1.2003944))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def PM_get_f0_and_formants(snd, pitchLo = 75, pitchHi = 400, pitchTimeStep = 30, FormantWindowLength = 30,\n",
    "                           maxNumberFormants = 4, maxFormantFreq = 4000):\n",
    "\n",
    "    # evaluate pitch using praat-parselmouth\n",
    "    PM_pitch = snd.to_pitch(pitch_floor = pitchLo, pitch_ceiling=pitchHi)\n",
    "\n",
    "    pitch_tVec = PM_pitch.ts()\n",
    "    pitchValues = np.zeros_like(pitch_tVec)\n",
    "\n",
    "    for timeIdx, time in enumerate(pitch_tVec):\n",
    "        pitchValues[timeIdx] = PM_pitch.get_value_at_time(time=time)\n",
    "\n",
    "    # evaluate formants using praat-parselmouth (here: BURG-algorithm is used)\n",
    "    PM_formants = snd.to_formant_burg(maximum_formant=maxFormantFreq, window_length=FormantWindowLength/1000,\n",
    "                                      max_number_of_formants=maxNumberFormants)\n",
    "\n",
    "    formant_tVec = PM_formants.ts()\n",
    "    formantValues = np.zeros((maxNumberFormants,formant_tVec.size))\n",
    "\n",
    "    for timeIdx, time in enumerate(formant_tVec):\n",
    "        for formantIdx in range(maxNumberFormants):\n",
    "            formantValues[formantIdx,timeIdx] = PM_formants.get_value_at_time(formant_number=formantIdx+1, time=time)\n",
    "    \n",
    "    return pitchValues, pitch_tVec, formantValues, formant_tVec\n",
    "\n",
    "# calculate spectrogram using praat-parselmouth\n",
    "spectroDataDB, spec_tVec, spec_fVec = PM_get_spectrogram(snd, 30, maximumFrequency)\n",
    "\n",
    "# calculate f0 and formants using praat-parselmouth\n",
    "pitchValues, pitch_tVec, formantValues, formant_tVec = PM_get_f0_and_formants(snd)\n",
    "\n",
    "# plot spectrogram with f0 and formants\n",
    "plottitle = \"Spectrogram, f0 and Formants of Sound Sample - File: \" + fileName\n",
    "SCplot.plot_spectrogram_with_formants(spectroDataDB, spec_tVec, spec_fVec, formantValues, formant_tVec,plottitle,\n",
    "                                      pitchValues=pitchValues, pitch_tVec=pitch_tVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formants F1 & F2 in the Vowel chart\n",
    "Especially with vowels, the first two formants F1 and F2 are very important in order to distinguish one vowel from another. Therefore, F1 and F2 can be displayed in the so-called vowel chart. Below is an interactive vowel chart where you can select a region in the audio file from which the detected formants are displayed in the vowel chart.\n",
    "The vowel chart used here is by Sendlmeier et. al - for more information see [here](https://www.kw.tu-berlin.de/fileadmin/a01311100/Formantkarten_des_deutschen_Vokalsystems_01.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dt_snd = snd.x_grid()[:-1]\n",
    "F1 = formantValues[0,:]\n",
    "F2 = formantValues[1,:]\n",
    "\n",
    "\n",
    "plottitle = \"Detected Formants F1 & F2 in the Vowel Chart - File: \" + fileName\n",
    "PM_timeWidget = SCplot.plot_F1_F2_in_vowel_chart(np.squeeze(snd.values), dt_snd, F1, F2, formant_tVec, plottitle)\n",
    "widgets.HBox([PM_timeWidget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Estimation of Vocal Tract using Cepstrum and LPC\n",
    "## LPC: Levinson Durbin Algorithm\n",
    "The Levinson.Durbin-Algorithm is an elegant prcedure to calculate the LPC-coefficients recursively. This implementation uses the whole signal for processing.\n",
    "\n",
    "### Task 3.1: What kind of soundfile is suitable for LPC analysis? How can sentences be analysed?\n",
    "###### Estimating the Vocal Tract Filter of speech signals (more generally: seperating the source signal from the transfer path) requires a stationary signal. If the transfer path changes inside the signal to analyse, the results are not representative. Therefore, sentences have to be buffered in order to analyse them!\n",
    "\n",
    "Choose a suitable sound file for the Levinson-Durbin-Algorithm from the code lines below, or choose another from the file directory. You can listen to a file by uncommenting the related line in the code below. \n",
    "\n",
    "Keep in mind that the whole signal is used for this particular implementation of the LPC. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an Audio File\n",
    "\n",
    "#fileName = 'f116.wav'\n",
    "#fileName = 'f216.wav'\n",
    "fileName = 'a_8000.wav'\n",
    "#fileName = '1000hz_3sec.wav'\n",
    "\n",
    "_, sig, fs, filePath = SCtools.import_sound_file(fileName)\n",
    "ipd.Audio(filePath) # show audio player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set the LPC order below (\"lpcOrder\"). If there are confusingly many coefficients plotted, choose a suitalbe number of coefficients to display (\"coefficientBoundary\"). Run the code and take a look at the results!\n",
    "\n",
    "You have the option to implement a pre-emphasis filter. Therefore call the function \"pre_emphasis_filtering(sig, fs)\" before starting the Levinson-Durbin iterations.\n",
    "\n",
    "### Task 3.2 Implement the pre-emphasis filter and compare the results. What does a pre-emphasis filter do, and why does the algorithm perform better/worse?\n",
    "###### Comparing the results, there is a significant drop to high frequencies in the vocal tract spectrum of the estimation without pre-emphasis. Also the autocorrelation of the pre-emphasis filtered signal drops to much smaller values. The results of the estimation with pre-emphasis seem more reliable.\n",
    "###### The pre-emphasis filter whitens the spectrum and therefore decorrelates the signal (for time-lags bigger than 0). The Levinson-Durbin Algorithm is constructed to perform best with decorrelated signals. In case of speech, the main signal energy is located in lower frequencies. Therefore the pre-emphasis filter is a low-cut filter, in this case a first order low-cut filter. \n",
    "\n",
    "In the resulting plot below, the iterations of the Levinson-Durbin algorithm can be stepped through. The approach of the coefficients can be observed.\n",
    "\n",
    "### Task 3.3 How can you explain the prominent peaks in the estimated vocal tract spectrum for high LPC orders?\n",
    "###### These peaks represent the harmonics of the speech signal. For too large numbers of coefficients these also get estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def autocorr(x, norm=True):\n",
    "# Calculate Autocorrelation:\n",
    "# input:\n",
    "#   x...input signal\n",
    "#   norm...(bool) normalize autocorrelation (default True)\n",
    "#\n",
    "# return:\n",
    "#   result...autocorrelation of x (only first half)\n",
    "#\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    result = result[result.size // 2:]\n",
    "    if norm:\n",
    "        result = result/result[0]\n",
    "    return result\n",
    "\n",
    "def pre_emphasis_filtering(sig, fs):\n",
    "# Pre-Emphasis Filtering:\n",
    "# input:\n",
    "#   sig...signal to filter\n",
    "#   fs...sampling frequency\n",
    "#\n",
    "# return:\n",
    "#   sigPreEmphasis...filtered signal\n",
    "#\n",
    "    fPreEmph = 10\n",
    "    # alpha is calculated as in egifa!\n",
    "    alpha = np.exp(-2*np.pi*fPreEmph/fs)\n",
    "    # 1. order Low-Cut\n",
    "    sigPreEmphasis = signal.lfilter(np.append([1], -alpha),[1],sig)\n",
    "    return sigPreEmphasis\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "lpcOrder = 100 # LPC order in samples\n",
    "coefficientBoundary = lpcOrder # max number of filter coefficients in plot\n",
    "\n",
    "E = np.zeros(lpcOrder-1) # define size of error vector\n",
    "K = np.zeros(lpcOrder-1) # define size of reflection coefficient vector\n",
    "a_all = np.zeros([lpcOrder-1, lpcOrder]) # define size of filter coefficient matrix (coefficient vector each row)\n",
    "\n",
    "sigPreEmphasis = pre_emphasis_filtering(sig, fs) # pre-emphasis filtering to whiten the speech signal before analyzing it\n",
    "\n",
    "R = autocorr(sigPreEmphasis) # calc. the autocorrelation of the signal (only significant part)\n",
    "#R = autocorr(sig) # calc. the autocorrelation of the signal (only significant part)\n",
    "SCplot.plot_autocorr(R, 'Auto-Correlation') # plot auto-correlation\n",
    "\n",
    "K[0] = R[1]/R[0] # initialize reflection coefficient\n",
    "a = K[0] # initialize lpc coefficient\n",
    "a_all[0,0] = 1\n",
    "a_all[0,1] = -a # save initial lpc coefficient as IIR filter coefficient\n",
    "E[0] = R[0] # initialize error\n",
    "\n",
    "for i in range(1,lpcOrder-1):\n",
    "    \n",
    "    K[i] = (R[i+1] - np.dot(a, R[1:i+1])) / E[i-1] # calc new reflection coefficient\n",
    "    \n",
    "    a = np.append(K[i], a-K[i]*np.flip(a)) # calc new lpc coefficients\n",
    "    \n",
    "    E[i] = E[i-1] * (1-(abs(K[i])**2)) # calc new error\n",
    "    \n",
    "    a_all[i,0:i+2] = np.append([1], -np.flip(a)) # save current lpc coefficients as IIR filter coefficients\n",
    "\n",
    "\n",
    "#a=a_all[-1] \n",
    "\n",
    "#SCplot.plot_filter(a, fs, 'Vocal Tract Filter - Frequenzy Response')\n",
    "#SCplot.plot_zplane([1], a, 'Vocal Tract Filter - Z Plane')\n",
    "\n",
    "SC_iterationWidget = SCplot.plot_interactive_filter_zplane(a_all, coefficientBoundary, fs, 'Vocal Tract Filter')\n",
    "widgets.HBox([SC_iterationWidget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below the time signals and spectra of the original signal and the signal inversly-filtered with the estimated vocal-tract (so the estimated glottis signal!) are displayed.\n",
    "\n",
    "With the plot above, select a reasonable iteration number and number of coefficients. By writing these parameters into the section below (\"selectedIteration\", \"limitCoefficients\"), the figures correspond to the selected filter.\n",
    "\n",
    "### Task 3.4 Zoom into the Glottis time signal and measure the period using the crosshairs. Then calculate the frequency of the Glottis signal and compare it to the frequency derived from the vocal-tract estimation.\n",
    "###### measured from Glottis signal: T = 0.008s    ->    f0 = 125Hz\n",
    "###### measured from Vocal Tract: f0 = 140Hz\n",
    "###### The measurement from the glottis signal is a local observation in contrast to the global observation of the estimated vocal tract.\n",
    "### Task 3.5 Think of applications for LPC!\n",
    "######  vocoder\n",
    "###### telephone\n",
    "###### speech analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calclate Spectrum:\n",
    "# input:\n",
    "#   signalData...input signal\n",
    "#   NFFT...fft order\n",
    "#   window...(bool) window signal with hann window (default True)\n",
    "#   logarithm...(bool) returns the spectrum in dB if true (default True)\n",
    "#\n",
    "# return:\n",
    "#   spetrum...positive frequencies of spectrum\n",
    "#\n",
    "def get_spectrum(signalData, NFFT, window = True, logarithm = True):\n",
    "    dataVec = np.zeros(NFFT)\n",
    "    if len(signalData) > NFFT:\n",
    "        dataVec = signalData[0:NFFT-1]\n",
    "        win = signal.hann(NFFT)\n",
    "    else:\n",
    "        dataVec[0:len(signalData)] = signalData\n",
    "        win = signal.hann(len(signalData))\n",
    "        win = np.append(win, np.zeros(NFFT-len(signalData)))\n",
    "    if window:\n",
    "        dataVec = dataVec*win\n",
    "    if logarithm:\n",
    "        spectrum = 20*np.log10(abs(fft(dataVec)))\n",
    "    else:\n",
    "        spectrum = abs(fft(dataVec))\n",
    "    return spectrum[0:round((len(spectrum)+1)/2)]\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "selectedIteration = 24 # select reasonalbe filter coefficients from the interactive plot\n",
    "limitCoefficients = 0 # limit number of filter coefficients (if 0, no limit)\n",
    "\n",
    "selectedFilter = np.trim_zeros(a_all[selectedIteration], 'b')\n",
    "if limitCoefficients < len(selectedFilter) and limitCoefficients > 0:\n",
    "    selectedFilter = selectedFilter[0:limitCoefficients]\n",
    "\n",
    "print(selectedFilter)\n",
    "\n",
    "filteredSignal = signal.lfilter([1], selectedFilter, sig)\n",
    "filteredSignal = filteredSignal / max(abs(filteredSignal)) # normalize\n",
    "inverseFiteredSignal = signal.lfilter(selectedFilter, [1], sig)\n",
    "inverseFiteredSignal = inverseFiteredSignal / max(abs(inverseFiteredSignal)) # normalize\n",
    "\n",
    "SCplot.plot_time_signal(sig, fs, 'Original Time Signal')\n",
    "#SCplot.plot_time_signal(filteredSignal, fs, 'Vocal Tract Time Signal')\n",
    "SCplot.plot_time_signal(inverseFiteredSignal, fs, 'Glottis Time Signal')\n",
    "\n",
    "sigSpectrum = get_spectrum(sig, len(sig), window=True)\n",
    "filteredSignalSpectrum = get_spectrum(filteredSignal, len(filteredSignal), window=True)\n",
    "inverseFiteredSignalSpectrum = get_spectrum(inverseFiteredSignal, len(inverseFiteredSignal), window=True)\n",
    "\n",
    "freqVector = np.linspace(0, fs/2, round(len(sig)/2+1))\n",
    "\n",
    "SCplot.plot_spectrum(get_spectrum(sig, len(sig), window=True), freqVector, 'Original Signal - Spectrum')\n",
    "#SCplot.plot_spectrum(get_spectrum(filteredSignal, len(filteredSignal), window=True), freqVector, 'Vocal Tract - Spectrum')\n",
    "SCplot.plot_spectrum(get_spectrum(inverseFiteredSignal, len(inverseFiteredSignal), window=True), freqVector, 'Glottis - Spectrum')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cepstrum Analysis\n",
    "In this part the source, filter seperation is carried out by a cepstral analysis.\n",
    "Therefore the cepstrum of the signal is calculated (LINK to doku).\n",
    "In the cepstrum the slowly changing parts of the spectrum are located in the lower quefrencies and the fast oscillating parts in higher quefrencies. Therefore we can calculate the spectral envelope by liftering the cepstrum with a rectangular lifter.\n",
    "\n",
    "### Task 3.7 Choose the lifter length (\"lifterLength\") in samples so a reasonalbe vocal tract filter results.\n",
    "\n",
    "### Task 3.8 Compare both vocal tract estimations and discus the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate Cepstrum:\n",
    "# input:\n",
    "#   spectrum...spectrum input\n",
    "#   logBase10...(bool) if true log10 is used, if false the natural log is used (default True)\n",
    "#   mirrorSpectrum...(bool) mirrors spectrum if treu, use for positive frequency input (default False)\n",
    "#\n",
    "# return:\n",
    "#   cepstrum...cepstrum of positive and negative quefrencies\n",
    "#\n",
    "def get_cepstrum(spectrum, logBase10 = True, mirrorSpectrum = False):\n",
    "    print('before', len(spectrum))\n",
    "    if mirrorSpectrum:\n",
    "        spectrum = np.append(spectrum, np.flip(spectrum[0:-1]))\n",
    "    print('after', len(spectrum))\n",
    "    if logBase10:\n",
    "        spectLog = np.log10(abs(spectrum))\n",
    "    else:\n",
    "        spectLog = np.log(abs(spectrum))\n",
    "    #cepstrum = 4* abs(ifft(spectLog))**2\n",
    "    cepstrum = abs(ifft(spectLog))\n",
    "    return cepstrum\n",
    "        \n",
    "###########################################################################################################################\n",
    "\n",
    "lifterLength = 15 # set length of cepstrum lifter\n",
    "\n",
    "sig = sig - np.mean(sig)\n",
    "\n",
    "sigLength = len(sig)\n",
    "fftLength = sigLength\n",
    "timeVector = np.linspace(0, sigLength/fs, sigLength)\n",
    "freqVector = np.linspace(0, fs/2, round(fftLength/2+1))\n",
    "\n",
    "sigEmph = pre_emphasis_filtering(sig, fs) # pre-emphasis filtering to whiten the speech signal before analyzing it\n",
    "\n",
    "SCplot.plot_spectrum(get_spectrum(sig, len(sig), window = True), freqVector, 'Signal Spectrum')\n",
    "\n",
    "spectrum = get_spectrum(sig, len(sig), window = True, logarithm=False)\n",
    "#spectrum = get_spectrum(sigEmph, len(sig), window = True, logarithm=False)\n",
    "\n",
    "cepstrum = get_cepstrum(spectrum, mirrorSpectrum=True, logBase10=True)\n",
    "SCplot.plot_cepstrum(20*np.log10(abs(cepstrum[0:round(len(cepstrum)/2+1)])), range(len(cepstrum[0:round(len(cepstrum)/2+1)])), lifterLength, 'Signal Cepstrum', lifterLP=True)\n",
    "cepstrum[lifterLength:-lifterLength] = 0 #liftering\n",
    "\n",
    "#inverseCepstrum = 10**(abs(fft(abs(cepstrum)**(1/2) / 4))) #for cepstrum calculated with log base of 10\n",
    "inverseCepstrum = 10**abs(fft(abs(cepstrum)))\n",
    "inverseCepstrum = inverseCepstrum[0:round(len(inverseCepstrum)/2)]\n",
    "\n",
    "\n",
    "SCplot.plot_spectrum(20*np.log10(inverseCepstrum), freqVector, 'Vocal Tract Spectrum')\n",
    "\n",
    "#gefiltert = np.divide(20*np.log10(spectrum), 20*np.log10(inverseCepstrum))\n",
    "gefiltert = 20*np.log10(spectrum) - 20*np.log10(inverseCepstrum)\n",
    "\n",
    "SCplot.plot_spectrum((gefiltert), freqVector, 'Glottis Signal Spectrum')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4: Formant Analysis\n",
    "\n",
    "In this exercise you need to record your own speech-samples. Before recording is possible the input/output sounddevices have to be set. Use the first cell to display all possible sound-devices and select the wanted devices by assigning the sounddevice-indices to the variables 'input_device' and 'output_device'.\n",
    "\n",
    "Then you can use the given record, play to record your own speech-samples.\n",
    "\n",
    "For this experiment please record 3 different versions of one sentence and analyze the time-domain signal and its formant and f0 structure (as already done in Experiment 2) and plot the results in separate cells. By plotting the results in separate cells it is possible to always use the same record button and audio-array for different recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fs = 44100 #Hz\n",
    "\n",
    "#show all possible sound-devices\n",
    "sd.query_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set default fs and number of channels\n",
    "sd.default.samplerate = fs\n",
    "num_channels = 2\n",
    "\n",
    "# select sound-device by choosing ID of above shown list.\n",
    "input_device = 0\n",
    "output_device = 1\n",
    "\n",
    "fs_target = 8000\n",
    "\n",
    "sd.default.device = [input_device,output_device]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record and playback audio\n",
    "toggleRec,togglePlay,out,box_layout,indata = SCtools.get_rec_and_play_button(fs, num_channels, max_duration = 30)\n",
    "\n",
    "widgets.HBox([toggleRec,togglePlay,out],layout=box_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Recording of Standard Sentence:\n",
    "Record a standard sentence and plot the time-domain signal and the spectrogram/formant/f0 plot as you have already done in Experiment 2. For the time-domain signal use the provided function 'get_plot_time_domain_sig()'. The function-arguments are described in the corresponding function's header available in the file 'plot_functions.py'. To analyze the spectrogram/frequency and f0 structure of the recorded sentence you can use the Parselmouth-analysis and the corresponding plot-functions of Experiment 2 ('PM_get_spetrogram()' and 'PM_get_f0_and_formants()'). Please plot the results in a separate Cell and answer the following questions?\n",
    "\n",
    "> - Are there visible formant contours?\n",
    ">>-    Answer: yes there should be visible formants assuming a standard sentence containing vowels was recorded.\n",
    "> - Can you distinguish between vowels, consonants and fricatives?\n",
    ">>-    Answer: Yes vowels are distinguishable with a clear formant structure and fricatives don't show a clear formant-structure. For consonants like 'l', 'm' or 'n' there can be visible formant structures because they can also be used in a voiced way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = SCtools.deleteNan(indata)\n",
    "#convert to mono\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "snd = parselmouth.Sound(recData)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plot time-domain signal   \n",
    "SCplot.get_plot_time_domain_sig(recData,rec_time,'Recorded Time-Domain-Signal',showPlot=True)\n",
    "\n",
    "# calculate spectrogram using praat-parselmouth\n",
    "speech_spectro, speech_spectro_t, speech_spectr_f = PM_get_spectrogram(snd, windowLengthMS=30, maximumFrequency=5000)\n",
    "\n",
    "# calculate f0 and formants using praat-parselmouth\n",
    "pitchValues, pitch_tVec, formantValues, formant_tVec = PM_get_f0_and_formants(snd)\n",
    "\n",
    "plottitle = 'Spectrogram, f0 and Formants of recorded Sample'\n",
    "SCplot.plot_spectrogram_with_formants(speech_spectro, speech_spectro_t, speech_spectr_f, formantValues, formant_tVec,plottitle,\n",
    "                                      pitchValues=pitchValues, pitch_tVec=pitch_tVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Recording of Dada-Sentence:\n",
    "Now, please go back to the \"record\" button and record a so called Dada-sentence but try to keep the same pitch/pitch course as in the sentence before. The Dada-sentence should not consist of words but rather of sounds like 'da', 'la', 'fa' etc. After recording please plot the time-domain signal and spectrogram/formant/f0-plot in a separate cell.\n",
    "> - Is the course of f0 the same as in the Recording before?\n",
    ">> - Answer: Should be the same if it has been possible to keep the same pitch as the sentence before, if not maybe try again.\n",
    "> - How do the formant-structures differ?\n",
    ">> - Answer: If only one sound e.g. 'da' was used the same formant-structure should repeat itself and it should be visible how the formants of the 'a' contained in 'da', emerge from lower frequencies, due to the fact that a consonant is used before the 'a'.\n",
    "> - Are there any differences visible in the time-domain?\n",
    ">> - Answer: If a voiced sound like 'da' was used to record the sentence the time-domain signal should consist only of voiced-segments and no \"noisy\" segments. \"Voiced\" segments contain more energy than \"noisy\" whereas noisy segments of sounds like 's' or the german 'sch' fluctuate more randomly and the typical noise structure is visible in the time-domain signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = SCtools.deleteNan(indata)\n",
    "#convert to mono\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "snd = parselmouth.Sound(recData)\n",
    "\n",
    "#plot time-domain signal\n",
    "SCplot.get_plot_time_domain_sig(recData,rec_time,'Recorded Time-Domain-Signal',showPlot=True)\n",
    "\n",
    "# calculate spectrogram using praat-parselmouth\n",
    "speech_spectro, speech_spectro_t, speech_spectr_f = PM_get_spectrogram(snd, windowLengthMS=30, maximumFrequency=5000)\n",
    "\n",
    "# calculate f0 and formants using praat-parselmouth\n",
    "pitchValues, pitch_tVec, formantValues, formant_tVec = PM_get_f0_and_formants(snd)\n",
    "\n",
    "plottitle = 'Spectrogram, f0 and Formants of recorded Sample'\n",
    "SCplot.plot_spectrogram_with_formants(speech_spectro, speech_spectro_t, speech_spectr_f, formantValues, formant_tVec,plottitle,\n",
    "                                      pitchValues=pitchValues, pitch_tVec=pitch_tVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Recording of whispered Sentence\n",
    "Record the previous sentence but use a whispery voice and once more please plot the time-domain signal and the spectrogram/formant/f0-plot in a separate cell.\n",
    "> - How does the formant-structure differ in comparison to the first Recording?\n",
    ">> - Answer: Formant-Tracker results might vary more due to less energy in the speech signal. But some sort of Formant-structure should still be visible. \n",
    "> - How and why does the f0-course differ from the previouse recordings?\n",
    ">> - Answer: F0-Tracking does not work. Whispery voice does not contain a f0 structure due to the fact that the glottis does not close when speaking with a whispery voice. The glottis closing impulses are no longer present aand the Excitation signal is now noise-like. The missing glottis closing impulses lead to a missing f0 perception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = SCtools.deleteNan(indata)\n",
    "#convert to mono\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "snd = parselmouth.Sound(recData)\n",
    "\n",
    "#plot time-domain signal \n",
    "SCplot.get_plot_time_domain_sig(recData,rec_time,'Recorded Time-Domain-Signal',showPlot=True)\n",
    "\n",
    "# calculate spectrogram using praat-parselmouth\n",
    "speech_spectro, speech_spectro_t, speech_spectr_f = PM_get_spectrogram(snd, windowLengthMS=30, maximumFrequency=5000)\n",
    "\n",
    "# calculate f0 and formants using praat-parselmouth\n",
    "pitchValues, pitch_tVec, formantValues, formant_tVec = PM_get_f0_and_formants(snd)\n",
    "\n",
    "plottitle = 'Spectrogram, f0 and Formants of recorded Sample'\n",
    "SCplot.plot_spectrogram_with_formants(speech_spectro, speech_spectro_t, speech_spectr_f, formantValues, formant_tVec,plottitle,\n",
    "                                      pitchValues=pitchValues, pitch_tVec=pitch_tVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.) Bonus Exercise: Record your own Formants\n",
    "Use the provided 'Record' and 'Play' Button to record your own formant samples and visualize the results with the help of the previously used plot function 'plot_F1_F2_in_vowel_chart()'.\n",
    "\n",
    "\n",
    ">Hint: Make shure to limit the recording towards the voiced parts to avoid wrong formant detections. To do so the time-widget included in 'plot_F1_F2_in_vowel_chart()' can be a very helpful tool. If the slider does not respond whilst dragging it try to run the cell again! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record and playback audio\n",
    "fs = 11025 # lower sampling rate in order to reduce computational effort. \n",
    "sd.default.samplerate = fs\n",
    "toggleRec,togglePlay,out,box_layout,indata = SCtools.get_rec_and_play_button(fs, num_channels, max_duration = 30)\n",
    "\n",
    "widgets.HBox([toggleRec,togglePlay,out],layout=box_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Recording of single vowel:\n",
    "Try to record simple formant samples which contains a single vowel ('a','e','i','o'...).\n",
    "> - Do the results match the given F1/F2 chart? Please plot a recorded vowel for which the analysis matches the given chart.\n",
    ">> - Answer: vowels like 'i' or 'e' have proven themselves to produce good results which match the given chart reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = SCtools.deleteNan(indata)\n",
    "#convert to mono\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "snd = parselmouth.Sound(recData,sampling_frequency=fs)\n",
    "# calculate f0 and formants using praat-parselmouth\n",
    "pitchValues, pitch_tVec, formantValues, formant_tVec = PM_get_f0_and_formants(snd)\n",
    "\n",
    "\n",
    "dt_snd = snd.x_grid()[:-1]\n",
    "F1 = formantValues[0,:]\n",
    "F2 = formantValues[1,:]\n",
    "\n",
    "\n",
    "plottitle = \"Recorded and detected Formants F1 & F2 in the Vowel Chart\"\n",
    "PM_timeWidget = SCplot.plot_F1_F2_in_vowel_chart(np.squeeze(snd.values), dt_snd, F1, F2, formant_tVec, plottitle)\n",
    "widgets.HBox([PM_timeWidget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Recording of a diphtong:\n",
    "Try to record a formant samples which contains a diphtong vowel ('ai','au','ei','ou'...).\n",
    "> - Again please visualize the tracked formants with the given plot-function in the F1/F2 vowel chart. Is the course of the changing formants visible within the vowel-chart?\n",
    ">> - Answer: Works very well if time range is set to limit sample towards voiced regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = SCtools.deleteNan(indata)\n",
    "#convert to mono\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "snd = parselmouth.Sound(recData,sampling_frequency=fs)\n",
    "# calculate f0 and formants using praat-parselmouth\n",
    "pitchValues, pitch_tVec, formantValues, formant_tVec = PM_get_f0_and_formants(snd)\n",
    "\n",
    "\n",
    "dt_snd = snd.x_grid()[:-1]\n",
    "F1 = formantValues[0,:]\n",
    "F2 = formantValues[1,:]\n",
    "\n",
    "\n",
    "plottitle = \"Recorded and detected Formants F1 & F2 in the Vowel Chart\"\n",
    "PM_timeWidget = SCplot.plot_F1_F2_in_vowel_chart(np.squeeze(snd.values), dt_snd, F1, F2, formant_tVec, plottitle)\n",
    "widgets.HBox([PM_timeWidget])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
