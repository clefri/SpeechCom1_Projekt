{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Load Modules and Import Audio\n",
    "\n",
    "### Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# uncomment the following 2 lines to reload the modules automatically,\n",
    "# such that changes to Plot_functions.py are reloaded without restarting the kernel!\n",
    "#\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#\n",
    "###########################################################################################\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "import soundfile as sf\n",
    "import bokeh\n",
    "import sounddevice as sd\n",
    "import time as clock\n",
    "#from plot_functions import SCplot # imports the necessary plot functions\n",
    "import Plot_functions as SCplot\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import signal, fft, ifft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Playback Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def import_sound_file(fileName, soundFolder=Path('../sounds/')):\n",
    "\n",
    "    filePath = soundFolder / fileName\n",
    "\n",
    "    audio1, fs = sf.read(filePath)\n",
    "    print(fileName+\" loaded with f_s ={}\".format(fs))\n",
    "\n",
    "    snd = parselmouth.Sound(str(filePath))\n",
    "    return snd, audio1, fs, str(filePath)\n",
    "          \n",
    "fileName = 'f116.wav'\n",
    "# fileName = 'f216.wav'\n",
    "\n",
    "snd, audio1, fs, filePath = import_sound_file(fileName)\n",
    "ipd.Audio(filePath) # show audio player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Time-Domain Analysis\n",
    "## Short-Time Average Energy (Intensity)\n",
    "\n",
    "To calculate the short-time average intensity, we implement the function SC_intensity(). In this function, the signal gets averaged using a Gaussian window. \n",
    "\n",
    "> TO DO: Convert the window length in milliseconds into a minimum pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "windowLength = 20 #millisecond\n",
    "## TODO FOR STUDENT: Calculate the minimum pitch from the window length\n",
    "#minimumPitch = ?\n",
    "minimumPitch = 1000/windowLength\n",
    "## END TODO FOR STUDENT\n",
    "\n",
    "def SC_intensity(sound, minimumPitch, fs):\n",
    "    #winLenEffective = np.round(3.2/minimumPitch * fs)  # Window length in samples; from Praat documentation; for pitch-synchronous intensity ripple\n",
    "    winLenEffective = np.round(1/minimumPitch * fs)\n",
    "    alpha = 2.5  # width factor alpha >= 0\n",
    "    std = (winLenEffective-1)/(2*alpha)  # Matlab documentation\n",
    "\n",
    "    win = signal.windows.kaiser(winLenEffective, 20/np.pi)\n",
    "    \n",
    "    sound = np.square(sound-np.mean(sound)) \n",
    "\n",
    "    intensity = np.convolve(sound, win, mode='valid') /np.sum(win)\n",
    "    \n",
    "    intensity = 10*np.log10(intensity/(4e-10)) # conversion to dB ; norm to (20 muPa)^2\n",
    "#     intensity = 10*np.log10(intensity/(1e-12)) # conversion to dB; norm to 1 pW/m2\n",
    "#     intensity = 20*np.log10(intensity/(2e-5)) # conversion to dB; norm to 20 muPa/m2\n",
    "    print(\"SC_intensity: Intensity has {} samples\".format(intensity.size))\n",
    "    winLenEffectiveTime = winLenEffective / fs\n",
    "    return intensity, win, winLenEffectiveTime\n",
    "\n",
    "SC_intensity, gaussWin, winLenEffectiveTime = SC_intensity(np.squeeze(snd.values), minimumPitch, fs)\n",
    "\n",
    "# Plot window function\n",
    "plottitle = \"Gaussian Window for Averaging with SC_intensity()\"\n",
    "dt_win = np.arange(0, gaussWin.size) / fs  # time axis for Gaussian Window\n",
    "p_window = SCplot.get_plot_window(gaussWin, dt_win, plottitle)\n",
    "\n",
    "# Plot intensity curve\n",
    "snd_values = np.squeeze(snd.values)\n",
    "dt_snd = np.arange(0, snd_values.size) / fs\n",
    "dt_SC_intensity = np.arange(0,SC_intensity.size) / fs \n",
    "dt_SC_intensity = dt_SC_intensity + winLenEffectiveTime/2 # shift to center the intensity curve bins in the windows\n",
    "\n",
    "plottitle = 'Intensity calculated with SC_intensity() - File: ' + fileName\n",
    "p_intensity = SCplot.get_plot_intensity(snd_values, dt_snd, SC_intensity, dt_SC_intensity, plottitle)\n",
    "\n",
    "SCplot.plot_in_subplots(p_window, p_intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the library praat-parselmouth provides us with functionality to calculate the intensity. To calculate the intensity with parselmouth, the member function to_intensity() is used, which takes the minimum pitch as an input argument. To compare parselmouth's to_intensity() with our custom SC_intensity(), we use the same input parameters as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PM_intensity = snd.to_intensity(minimum_pitch=minimumPitch, subtract_mean=False)  # intensity calculation with parselmouth's function\n",
    "PM_intensity = snd.to_intensity(minimum_pitch=minimumPitch, subtract_mean=False, time_step=1/fs)  # intensity calculation with parselmouth's function\n",
    "print(\"PM_intensity: Intensity has {} samples\".format(PM_intensity.get_number_of_frames()))\n",
    "\n",
    "dt_PM_intensity = PM_intensity.x_grid()[:-1]\n",
    "PM_intensity_val = np.squeeze(PM_intensity.values)\n",
    "dt_snd = snd.x_grid()[:-1]\n",
    "\n",
    "plottitle = \"Intensity calculated with parselmouth's to_intensity() - File: \" + fileName\n",
    "SCplot.get_plot_intensity(np.squeeze(snd.values), dt_snd, PM_intensity_val, dt_PM_intensity, plottitle, showPlot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the two intensity curves in the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 2 intensity curves in one plot\n",
    "plottitle = \"Comparison of both intensity curves - File: \" + fileName\n",
    "SCplot.plot_two_intensity_curves(dt_PM_intensity, PM_intensity_val, dt_SC_intensity, SC_intensity, plottitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Describe the key differences of the intensity curve calculated with your own function SC_intensity() to parselmouth's to_intensity().\n",
    "#### Expected Answers:\n",
    "- different value range\n",
    "- parselmouths intensity is more sparsely sampled\n",
    "- parselmouths intensity has not the same time range as the audio file, whereas the custom intensity covers the whole time range of the audio file\n",
    "\n",
    "### Task 2: Is there a way to alter the intensity curve calculated with your own function SC_intensity(), such that it matches the intensity curve calculated with parselmouth better?\n",
    "> Hint: see <https://www.fon.hum.uva.nl/praat/manual/Sound__To_Intensity___.html>\n",
    "#### Expected Answer:\n",
    "- the student should recognize that parselmouth uses an effective window length that is 3.2 times the original window length\n",
    "- the student should modify the function SC_intensity() such that it uses this effective window length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Frequency-Domain Analysis\n",
    "### Load and Playback Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an Audio File\n",
    "fileName = 'f116.wav'\n",
    "#fileName = 'f216.wav'\n",
    "#fileName = 'a_8000.wav'\n",
    "#fileName = '1000hz_3sec.wav'\n",
    "\n",
    "snd, audio1, fs, filePath = import_sound_file(fileName)\n",
    "ipd.Audio(filePath) # show audio player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide- and narrow-band spectrograms\n",
    "First, we start by calculating a spectrogram using the method scipy.signal.spectrogram()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "windowlengthSec = 30 #ms\n",
    "windowlength = np.round(fs * windowlengthSec/1000).astype(int)\n",
    "#windowlength = 2048\n",
    "print('Window Length in samples:', windowlength)\n",
    "#overlap = windowlength-1\n",
    "overlap = np.round(windowlength / 2)\n",
    "\n",
    "#window = 'hann'\n",
    "std = (windowlength - 1)/(2*2.5)  # Matlab documentation\n",
    "window = ('gaussian', std)\n",
    "\n",
    "SC_fVec, SC_tVec, SC_spectroData = signal.spectrogram(audio1, fs=fs, window=window, noverlap=overlap, nperseg=windowlength, return_onesided=True, scaling='spectrum', mode='magnitude')\n",
    "\n",
    "SC_spectroDataDB = 20*np.log10(SC_spectroData / np.max(SC_spectroData))\n",
    "\n",
    "# plot interactive spectrogram\n",
    "plottitle = \"Custom Spectrogram of Sound Sample - File: \" + fileName\n",
    "SC_timeWidget = SCplot.plot_interactive_spectrogram(SC_spectroDataDB, SC_tVec, SC_fVec, plottitle)\n",
    "widgets.HBox([SC_timeWidget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use parselmouths to_spectrogram() to calculate a spectrogram. For Plotting the spectrogram, we use our custum function plot_interactive_spectrogram()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "windowlengthSec = 30 #ms\n",
    "maximumFrequency = 5000\n",
    "\n",
    "def PM_get_spectrogram(snd, windowLengthMS, maximumFrequency):\n",
    "    spectrogram = snd.to_spectrogram(window_length=windowLengthMS/1000, maximum_frequency=maximumFrequency)\n",
    "    PM_spectroData = spectrogram.values\n",
    "    PM_tVec = spectrogram.ts()\n",
    "    PM_fVec = spectrogram.ys()\n",
    "    PM_spectroDataDB = 10*np.log10(PM_spectroData / np.max(PM_spectroData))\n",
    "    return PM_spectroDataDB, PM_tVec, PM_fVec\n",
    "\n",
    "PM_spectroDataDB, PM_tVec, PM_fVec = PM_get_spectrogram(snd, windowlengthSec, maximumFrequency)\n",
    "\n",
    "plottitle = \"Parselmouth Spectrogram of Sound Sample - File: \" + fileName\n",
    "PM_timeWidget = SCplot.plot_interactive_spectrogram(PM_spectroDataDB, PM_tVec, PM_fVec, plottitle)\n",
    "widgets.HBox([PM_timeWidget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f0 and Formants\n",
    "To analyze the Formants, we use praat-parselmouths formant analysis methods.\n",
    "\n",
    "Select sound file to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an Audio File\n",
    "fileName = 'f116.wav'\n",
    "#fileName = 'f216.wav'\n",
    "#fileName = 'a_8000.wav'\n",
    "#fileName = '1000hz_3sec.wav'\n",
    "\n",
    "snd, _, fs, filePath = import_sound_file(fileName)\n",
    "ipd.Audio(filePath) # show audio player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we analyse the formants F1 ... F4 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowLength = 30 # ms\n",
    "maxNumberFormants = 4\n",
    "maxFormantFreq = 4000\n",
    "\n",
    "PM_formants = snd.to_formant_burg(maximum_formant=maxFormantFreq, window_length=windowLength/1000,\n",
    "                                  max_number_of_formants=maxNumberFormants)\n",
    "\n",
    "formant_tVec = PM_formants.ts()\n",
    "formantValues = np.zeros((maxNumberFormants,formant_tVec.size))\n",
    "\n",
    "for timeIdx, time in enumerate(formant_tVec):\n",
    "    for formantIdx in range(maxNumberFormants):\n",
    "        formantValues[formantIdx,timeIdx] = PM_formants.get_value_at_time(formant_number=formantIdx+1, time=time)\n",
    "\n",
    "spectroDataDB, spec_tVec, spec_fVec = PM_get_spectrogram(snd, 30, maximumFrequency)\n",
    "\n",
    "# plot spectrogram with formants\n",
    "plottitle = \"Spectrogram and Formants of Sound Sample - File: \" + fileName\n",
    "SCplot.plot_spectrogram_with_formants(spectroDataDB, spec_tVec, spec_fVec, formantValues, formant_tVec, plottitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we analyse the fundamental frequency f0 and additionally the formants F1, ..., F4.\n",
    "\n",
    "Select sound file to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an Audio File\n",
    "fileName = 'f116.wav'\n",
    "#fileName = 'f216.wav'\n",
    "#fileName = 'a_8000.wav'\n",
    "#fileName = '1000hz_3sec.wav'\n",
    "\n",
    "snd, _, fs, filePath = import_sound_file(fileName)\n",
    "ipd.Audio(filePath) # show audio player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pitchLo = 75 #Hz\n",
    "pitchHi = 400 #Hz\n",
    "pitchTimeStep = 30 #ms\n",
    "\n",
    "PM_pitch = snd.to_pitch(pitch_floor = pitchLo, pitch_ceiling=pitchHi)\n",
    "\n",
    "pitch_tVec = PM_pitch.ts()\n",
    "pitchValues = np.zeros_like(pitch_tVec)\n",
    "\n",
    "for timeIdx, time in enumerate(pitch_tVec):\n",
    "    pitchValues[timeIdx] = PM_pitch.get_value_at_time(time=time)\n",
    "\n",
    "# plot spectrogram with f0 and formants\n",
    "plottitle = \"Spectrogram, f0 and Formants of Sound Sample - File: \" + fileName\n",
    "SCplot.plot_spectrogram_with_formants(spectroDataDB, spec_tVec, spec_fVec, formantValues, formant_tVec,plottitle,\n",
    "                                      pitchValues=pitchValues, pitch_tVec=pitch_tVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Estimation of Vocal Tract using Cepstrum and LPC\n",
    "## LPC: Levinson Durbin Algorithm\n",
    "The Levinson.Durbin-Algorithm is an elegant prcedure to calculate the LPC-coefficients recursively. This implementation uses the whole signal for processing.\n",
    "\n",
    "### Task 3.1: What kind of soundfile is suitable for LPC analysis? How can sentences be analysed?\n",
    "###### Estimating the Vocal Tract Filter of speech signals (more generally: seperating the source signal from the transfer path) requires a stationary signal. If the transfer path changes inside the signal to analyse, the results are not representative. Therefore, sentences have to be buffered in order to analyse them!\n",
    "\n",
    "Choose a suitable sound file for the Levinson-Durbin-Algorithm from the code lines below, or choose another from the file directory. You can listen to a file by uncommenting the related line in the code below. \n",
    "\n",
    "Keep in mind that the whole signal is used for this particular implementation of the LPC. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an Audio File\n",
    "\n",
    "#fileName = 'f116.wav'\n",
    "#fileName = 'f216.wav'\n",
    "fileName = 'a_8000.wav'\n",
    "#fileName = '1000hz_3sec.wav'\n",
    "\n",
    "_, sig, fs, filePath = import_sound_file(fileName)\n",
    "ipd.Audio(filePath) # show audio player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set the LPC order below (\"lpcOrder\"). If there are confusingly many coefficients plotted, choose a suitalbe number of coefficients to display (\"coefficientBoundary\"). Run the code and take a look at the results!\n",
    "\n",
    "You have the option to implement a pre-emphasis filter. Therefore call the function \"pre_emphasis_filtering(sig, fs)\" before starting the Levinson-Durbin iterations.\n",
    "\n",
    "### Task 3.2 Implement the pre-emphasis filter and compare the results. What does a pre-emphasis filter do, and why does the algorithm perform better/worse?\n",
    "###### Comparing the results, there is a significant drop to high frequencies in the vocal tract spectrum of the estimation without pre-emphasis. Also the autocorrelation of the pre-emphasis filtered signal drops to much smaller values. The results of the estimation with pre-emphasis seem more reliable.\n",
    "###### The pre-emphasis filter whitens the spectrum and therefore decorrelates the signal (for time-lags bigger than 0). The Levinson-Durbin Algorithm is constructed to perform best with decorrelated signals. In case of speech, the main signal energy is located in lower frequencies. Therefore the pre-emphasis filter is a low-cut filter, in this case a first order low-cut filter. \n",
    "\n",
    "In the resulting plot below, the iterations of the Levinson-Durbin algorithm can be stepped through. The approach of the coefficients can be observed.\n",
    "\n",
    "### Task 3.3 Set the LPC order to a large number (arround 1000) and limit the displayed coefficients to a reasonable amount. Step through the iterations and note any irregularities, for the estimation without pre-emphasis filtering.\n",
    "###### no pre-emphasis: The estmation of the vocal tract is not representing the vocal tract for a large number of iterations. Some filter coefficient pole pairs converge from outside the unit circle and are therefore instable for the converging duration.\n",
    "\n",
    "### Task 3.4 How can you explain the prominent peaks in the estimated vocal tract spectrum for high LPC orders?\n",
    "###### These peaks represent the harmonics of the speech signal. For too large numbers of coefficients these also get estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def autocorr(x, norm=True):\n",
    "# Calculate Autocorrelation:\n",
    "# input:\n",
    "#   x...input signal\n",
    "#   norm...(bool) normalize autocorrelation (default True)\n",
    "#\n",
    "# return:\n",
    "#   result...autocorrelation of x (only first half)\n",
    "#\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    result = result[result.size // 2:]\n",
    "    if norm:\n",
    "        result = result/result[0]\n",
    "    return result\n",
    "\n",
    "def pre_emphasis_filtering(sig, fs):\n",
    "# Pre-Emphasis Filtering:\n",
    "# input:\n",
    "#   sig...signal to filter\n",
    "#   fs...sampling frequency\n",
    "#\n",
    "# return:\n",
    "#   sigPreEmphasis...filtered signal\n",
    "#\n",
    "    fPreEmph = 10\n",
    "    # alpha is calculated as in egifa!\n",
    "    alpha = np.exp(-2*np.pi*fPreEmph/fs)\n",
    "    # 1. order Low-Cut\n",
    "    sigPreEmphasis = signal.lfilter(np.append([1], -alpha),[1],sig)\n",
    "    return sigPreEmphasis\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "lpcOrder = 100 # LPC order in samples\n",
    "coefficientBoundary = lpcOrder # max number of filter coefficients in plot\n",
    "\n",
    "E = np.zeros(lpcOrder-1) # define size of error vector\n",
    "K = np.zeros(lpcOrder-1) # define size of reflection coefficient vector\n",
    "a_all = np.zeros([lpcOrder-1, lpcOrder]) # define size of filter coefficient matrix (coefficient vector each row)\n",
    "\n",
    "sigPreEmphasis = pre_emphasis_filtering(sig, fs) # pre-emphasis filtering to whiten the speech signal before analyzing it\n",
    "\n",
    "R = autocorr(sigPreEmphasis) # calc. the autocorrelation of the signal (only significant part)\n",
    "#R = autocorr(sig) # calc. the autocorrelation of the signal (only significant part)\n",
    "SCplot.plot_autocorr(R, 'Auto-Correlation') # plot auto-correlation\n",
    "\n",
    "K[0] = R[1]/R[0] # initialize reflection coefficient\n",
    "a = K[0] # initialize lpc coefficient\n",
    "a_all[0,0] = 1\n",
    "a_all[0,1] = -a # save initial lpc coefficient as IIR filter coefficient\n",
    "E[0] = R[0] # initialize error\n",
    "\n",
    "for i in range(1,lpcOrder-1):\n",
    "    \n",
    "    K[i] = (R[i+1] - np.dot(a, R[1:i+1])) / E[i-1] # calc new reflection coefficient\n",
    "    \n",
    "    a = np.append(K[i], a-K[i]*np.flip(a)) # calc new lpc coefficients\n",
    "    \n",
    "    E[i] = E[i-1] * (1-(abs(K[i])**2)) # calc new error\n",
    "    \n",
    "    a_all[i,0:i+2] = np.append([1], -np.flip(a)) # save current lpc coefficients as IIR filter coefficients\n",
    "\n",
    "\n",
    "#a=a_all[-1] \n",
    "\n",
    "#SCplot.plot_filter(a, fs, 'Vocal Tract Filter - Frequenzy Response')\n",
    "#SCplot.plot_zplane([1], a, 'Vocal Tract Filter - Z Plane')\n",
    "\n",
    "SC_iterationWidget = SCplot.plot_interactive_filter_zplane(a_all, coefficientBoundary, fs, 'Vocal Tract Filter')\n",
    "widgets.HBox([SC_iterationWidget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below the time signals and spectra of the original signal and the signal inversly-filtered with the estimated vocal-tract (so the estimated glottis signal!) are displayed.\n",
    "\n",
    "With the plot above, select a reasonable iteration number and number of coefficients. By writing these parameters into the section below (\"selectedIteration\", \"limitCoefficients\"), the figures correspond to the selected filter.\n",
    "\n",
    "### Task 3.5 Zoom into the Glottis time signal and measure the period using the crosshairs. Then calculate the frequency of the Glottis signal and compare it to the frequency derived from the vocal-tract estimation.\n",
    "###### measured from Glottis signal: T = 0.008s    ->    f0 = 125Hz\n",
    "###### measured from Vocal Tract: f0 = 140Hz\n",
    "###### The measurement from the glottis signal is a local observation in contrast to the global observation of the estimated vocal tract.\n",
    "### Task 3.6 Think of applications for low, midrange and high LPC orders (and therefore number of coefficients)!\n",
    "###### a low number of coefficients means high calculating efficiency but low vocal tract resolution -> vocoder\n",
    "###### a medium number of filter coefficients comes with larger computational cost but also better represents the real vocal tract -> telephone\n",
    "###### a large number of coefficients result in a longer caculation time but a very good estimation of the vocal tract -> speech analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calclate Spectrum:\n",
    "# input:\n",
    "#   signalData...input signal\n",
    "#   NFFT...fft order\n",
    "#   window...(bool) window signal with hann window (default True)\n",
    "#   logarithm...(bool) returns the spectrum in dB if true (default True)\n",
    "#\n",
    "# return:\n",
    "#   spetrum...positive frequencies of spectrum\n",
    "#\n",
    "def get_spectrum(signalData, NFFT, window = True, logarithm = True):\n",
    "    dataVec = np.zeros(NFFT)\n",
    "    if len(signalData) > NFFT:\n",
    "        dataVec = signalData[0:NFFT-1]\n",
    "        win = signal.hann(NFFT)\n",
    "    else:\n",
    "        dataVec[0:len(signalData)] = signalData\n",
    "        win = signal.hann(len(signalData))\n",
    "        win = np.append(win, np.zeros(NFFT-len(signalData)))\n",
    "    if window:\n",
    "        dataVec = dataVec*win\n",
    "    if logarithm:\n",
    "        spectrum = 20*np.log10(abs(fft(dataVec)))\n",
    "    else:\n",
    "        spectrum = abs(fft(dataVec))\n",
    "    return spectrum[0:round((len(spectrum)+1)/2)]\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "selectedIteration = 37 # select reasonalbe filter coefficients from the interactive plot\n",
    "limitCoefficients = 0 # limit number of filter coefficients (if 0, no limit)\n",
    "\n",
    "selectedFilter = np.trim_zeros(a_all[selectedIteration], 'b')\n",
    "if limitCoefficients < len(selectedFilter) and limitCoefficients > 0:\n",
    "    selectedFilter = selectedFilter[0:limitCoefficients]\n",
    "\n",
    "print(selectedFilter)\n",
    "\n",
    "filteredSignal = signal.lfilter([1], selectedFilter, sig)\n",
    "filteredSignal = filteredSignal / max(abs(filteredSignal)) # normalize\n",
    "inverseFiteredSignal = signal.lfilter(selectedFilter, [1], sig)\n",
    "inverseFiteredSignal = inverseFiteredSignal / max(abs(inverseFiteredSignal)) # normalize\n",
    "\n",
    "SCplot.plot_time_signal(sig, fs, 'Original Time Signal')\n",
    "#SCplot.plot_time_signal(filteredSignal, fs, 'Vocal Tract Time Signal')\n",
    "SCplot.plot_time_signal(inverseFiteredSignal, fs, 'Glottis Time Signal')\n",
    "\n",
    "sigSpectrum = get_spectrum(sig, len(sig), window=True)\n",
    "filteredSignalSpectrum = get_spectrum(filteredSignal, len(filteredSignal), window=True)\n",
    "inverseFiteredSignalSpectrum = get_spectrum(inverseFiteredSignal, len(inverseFiteredSignal), window=True)\n",
    "\n",
    "freqVector = np.linspace(0, fs/2, round(len(sig)/2+1))\n",
    "\n",
    "SCplot.plot_spectrum(get_spectrum(sig, len(sig), window=True), freqVector, 'Original Signal - Spectrum')\n",
    "#SCplot.plot_spectrum(get_spectrum(filteredSignal, len(filteredSignal), window=True), freqVector, 'Vocal Tract - Spectrum')\n",
    "SCplot.plot_spectrum(get_spectrum(inverseFiteredSignal, len(inverseFiteredSignal), window=True), freqVector, 'Glottis - Spectrum')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cepstrum Analysis\n",
    "In this part the source, filter seperation is carried out by a cepstral analysis.\n",
    "Therefore the cepstrum of the signal is calculated (LINK to doku).\n",
    "In the cepstrum the slowly changing parts of the spectrum are located in the lower quefrencies and the fast oscillating parts in higher quefrencies. Therefore we can calculate the spectral envelope by liftering the cepstrum with a rectangular lifter.\n",
    "\n",
    "### Task 3.7 Choose the lifter length (\"lifterLength\") in samples so a reasonalbe vocal tract filter results.\n",
    "\n",
    "### Task 3.8 Compare both vocal tract estimations and discus the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate Cepstrum:\n",
    "# input:\n",
    "#   spectrum...spectrum input\n",
    "#   logBase10...(bool) if true log10 is used, if false the natural log is used (default True)\n",
    "#   mirrorSpectrum...(bool) mirrors spectrum if treu, use for positive frequency input (default False)\n",
    "#\n",
    "# return:\n",
    "#   cepstrum...cepstrum of positive and negative quefrencies\n",
    "#\n",
    "def get_cepstrum(spectrum, logBase10 = True, mirrorSpectrum = False):\n",
    "    print('before', len(spectrum))\n",
    "    if mirrorSpectrum:\n",
    "        spectrum = np.append(spectrum, np.flip(spectrum[0:-1]))\n",
    "    print('after', len(spectrum))\n",
    "    if logBase10:\n",
    "        spectLog = np.log10(abs(spectrum))\n",
    "    else:\n",
    "        spectLog = np.log(abs(spectrum))\n",
    "    #cepstrum = 4* abs(ifft(spectLog))**2\n",
    "    cepstrum = abs(ifft(spectLog))\n",
    "    return cepstrum\n",
    "        \n",
    "###########################################################################################################################\n",
    "\n",
    "lifterLength = 15 # set length of cepstrum lifter\n",
    "\n",
    "sigLength = len(sig)\n",
    "fftLength = sigLength\n",
    "timeVector = np.linspace(0, sigLength/fs, sigLength)\n",
    "freqVector = np.linspace(0, fs/2, round(fftLength/2+1))\n",
    "\n",
    "sigEmph = pre_emphasis_filtering(sig, fs) # pre-emphasis filtering to whiten the speech signal before analyzing it\n",
    "\n",
    "SCplot.plot_spectrum(get_spectrum(sig, len(sig), window = True), freqVector, 'Signal Spectrum')\n",
    "\n",
    "spectrum = get_spectrum(sig, len(sig), window = True, logarithm=False)\n",
    "#spectrum = get_spectrum(sigEmph, len(sig), window = True, logarithm=False)\n",
    "\n",
    "cepstrum = get_cepstrum(spectrum, mirrorSpectrum=True, logBase10=True)\n",
    "SCplot.plot_cepstrum(20*np.log10(abs(cepstrum[0:round(len(cepstrum)/2+1)])), range(len(cepstrum[0:round(len(cepstrum)/2+1)])), lifterLength, 'Signal Cepstrum', lifterLP=True)\n",
    "cepstrum[lifterLength:-lifterLength] = 0 #liftering\n",
    "\n",
    "#inverseCepstrum = 10**(abs(fft(abs(cepstrum)**(1/2) / 4))) #for cepstrum calculated with log base of 10\n",
    "inverseCepstrum = 10**abs(fft(abs(cepstrum)))\n",
    "inverseCepstrum = inverseCepstrum[0:round(len(inverseCepstrum)/2)]\n",
    "\n",
    "\n",
    "SCplot.plot_spectrum(20*np.log10(inverseCepstrum), freqVector, 'Vocal Tract Spectrum')\n",
    "\n",
    "#gefiltert = np.divide(20*np.log10(spectrum), 20*np.log10(inverseCepstrum))\n",
    "gefiltert = 20*np.log10(spectrum) - 20*np.log10(inverseCepstrum)\n",
    "\n",
    "SCplot.plot_spectrum((gefiltert), freqVector, 'Glottis Signal Spectrum')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4: Formant Analysis\n",
    "\n",
    "In this exercise you need to record your own speech-samples. Before recording is possible the input/output sounddevices have to be set. Use the first cell to display all possible sound-devices and select the wanted devices by assigning the sounddevice-indices to the variables 'input_device' and 'output_device'.\n",
    "\n",
    "Then you can use the given record, play and clear buttons to record your own speech-samples.\n",
    "\n",
    "For this experiment please record 3 different versions of one sentence and analyze the time-domain signal and its formant and f0 structure (as already done in Experiment 2) and plot the results in separate cells. By plotting the results in separate cells it is possible to always use the same record button and audio-array for different recordings.\n",
    "\n",
    "## 1.) Recording:\n",
    "Record a standard sentence and plot the time-domain signal and the spectrogram/formant/f0 plot as you have already done in Experiment 2. For the time-domain signal use the provided function 'get_plot_time_domain_sig()'. The function-arguments are described in the corresponding function header. To analyze the spectrogram/frequency and f0 structure of the recorded sentence you can use the Parselmouth-analysis and the corresponding plot-functions of Experiment 2. Please plot the results in a separate Cell. \n",
    "\n",
    "> - Are there visible formant contours?\n",
    ">>-    Answer: yes there should be visible formants assuming a standard sentence containing vowels was recorded.\n",
    "> - Can you distinguish between vowels, consonants and fricatives?\n",
    ">>-    Answer: Yes vowels are distinguishable with a clear formant structure and fricatives don't show a clear formant-structure. For consonants like 'l', 'm' or 'n' there can be visible formant structures because they can also be used in a voiced way.\n",
    "\n",
    "## 2.) Recording:\n",
    "Record a so called Dada-sentence but try to keep the same pitch/pitch course as in the sentence before. The Dada-sentence should not consist of words but rather of sounds like 'da', 'la', 'fa' etc. (or maybe even a combination of meaningless sounds?). After recording please plot the time-domain signal and spectrogram/formant/f0-plot in a separate cell.\n",
    "> - Is the course of f0 the same as in the Recording before?\n",
    ">> - Answer: Should be the same if it has been possible to keep the same pitch as the sentence before, if not maybe try again.\n",
    "> - How do the formant-structures differ?\n",
    ">> - Answer: If only one sound e.g. 'da' was used the same formant-structure should repeat itself. Small deviations in the formant-structures are possible due to a certain dependency of the formants towards f0.\n",
    "> - Are there any differences visible in the time-domain?\n",
    ">> - Answer: If a voiced sound like 'da' was used to record the sentence the time-domain signal should consist only of voiced-segments and no \"noisy\" segments.\n",
    "\n",
    "## 3.) Recording:\n",
    "Record the previous sentence but use a whispery voice and once more please plot the time-domain signal and the spectrogram/formant/f0-plot in a separate cell.\n",
    "> - How does the formant-structure differ in comparison to the first Recording?\n",
    ">> - Answer: Formant-Tracker results might vary more due to less energy in the speech signal. But some sort of Formant-structure should still be visible. \n",
    "> - How and why does the f0-course differ from the previouse recordings?\n",
    ">> - Answer: F0-Tracking does not work. Whispery voice does not contain a f0 structure due to the fact that the glottis does not close properly when speaking with a whispery voice. The glottis closing impulses determine the f0 we percieve in a speech signal. The missing or weakened glottis closing impulses lead to a missing f0 perception.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fs = 44100 #Hz\n",
    "\n",
    "#show all possible sound-devices\n",
    "sd.query_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set default fs and number of channels\n",
    "sd.default.samplerate = fs\n",
    "num_channels = 2\n",
    "\n",
    "# select sound-device by choosing ID of above shown list.\n",
    "input_device = 1\n",
    "output_device = 3\n",
    "\n",
    "fs_target = 8000\n",
    "\n",
    "sd.default.device = [input_device,output_device]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_click_Rec(button):\n",
    "#callback function for Rec-Toggle button. Determines what happens if Rec-Button is switched On/Off\n",
    "    value = button.new\n",
    "    if (button.new == True):\n",
    "        button.owner.button_style='danger'\n",
    "        with out:\n",
    "            print('Recording started!')\n",
    "        sd.rec(out=indata,samplerate=fs)\n",
    "\n",
    "    else:\n",
    "        button.owner.button_style=''\n",
    "        sd.stop()\n",
    "        with out:\n",
    "            print('Recording stopped!')\n",
    "\n",
    "def deleteNan(indata):\n",
    "    # helper function to delete Nans in record-array (indata). Trims data-vector to all values which are NOT Nan.\n",
    "    indata_no_Nan = np.zeros([np.sum(~np.isnan(indata[:,0])),indata.shape[1]])\n",
    "    indata_no_Nan[:,0]=indata[~np.isnan(indata[:,0]),0]\n",
    "    indata_no_Nan[:,1]=indata[~np.isnan(indata[:,0]),1]\n",
    "    return indata_no_Nan\n",
    "        \n",
    "def on_click_Play(button):\n",
    "    #callback function for Play-Toggle button. Determines what happens if Play-Button is switched On/Off\n",
    "    if (button.new == True):\n",
    "        button.owner.button_style='Success'\n",
    "        with out:\n",
    "            print('Playback started!')\n",
    "        indata_no_Nan = deleteNan(indata)\n",
    "        sd.play(indata_no_Nan)\n",
    "        button.owner.description = 'Stop'\n",
    "        button.owner.icon = 'stop-circle'\n",
    "    if (button.new == False):\n",
    "        sd.stop()\n",
    "        with out:\n",
    "            print('Playback stoped!')\n",
    "        button.owner.button_style = ''\n",
    "        button.owner.description = 'Play'\n",
    "        button.owner.icon = 'play-circle'\n",
    "        \n",
    "def on_click_Clear(button):\n",
    "    indata[:] = np.NaN\n",
    "    with out:\n",
    "        ipd.clear_output()\n",
    "#        print('Audio-Array has been cleared!')\n",
    "    return indata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#maximum-duration for recorded samples ==> recorded sample shouldn't be so long (nothing is recorded after max_duration)\n",
    "max_duration = 30 # seconds\n",
    "indata = np.empty([max_duration*fs,num_channels])\n",
    "indata[:] = np.NaN\n",
    "\n",
    "\n",
    "toggleRec =widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Recording',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Record_Button',\n",
    "    icon='circle' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "togglePlay = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Play',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Play_Button',\n",
    "    icon='play-circle' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "clearButton = widgets.Button(\n",
    "    description='Clear Audio-Array',\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Clear_Button',\n",
    "    icon='trash' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "toggleRec.observe(on_click_Rec, 'value')\n",
    "\n",
    "togglePlay.observe(on_click_Play, 'value')\n",
    "\n",
    "clearButton.on_click(on_click_Clear)\n",
    "\n",
    "out = widgets.Output()\n",
    "#display(out)\n",
    "\n",
    "# object_methods = [method_name for method_name in dir(clearButton)\n",
    "#                  if callable(getattr(clearButton, method_name))]\n",
    "# print(object_methods)\n",
    "\n",
    "\n",
    "box_layout = widgets.Layout(display='flex',\n",
    "                flex_flow='column',\n",
    "                align_items='center',\n",
    "                width='100%')\n",
    "\n",
    "widgets.HBox([toggleRec,togglePlay,clearButton,out],layout=box_layout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Plot: Recording of Standard Sentence\n",
    ">-time-domain signal plot\n",
    "\n",
    ">-Spectrogram/Formant/f0 plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = deleteNan(indata)\n",
    "#convert to mono\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "\n",
    "#plot time-domain signal   \n",
    "SCplot.get_plot_time_domain_sig(recData,rec_time,'Recorded Time-Domain-Signal',showPlot=True)\n",
    "\n",
    "# Analyze Formants and f0 with parselmouth:\n",
    "pitchLo = 75 #Hz\n",
    "pitchHi = 400 #Hz\n",
    "pitchTimeStep = 30 #ms\n",
    "\n",
    "snd = parselmouth.Sound(recData)\n",
    "\n",
    "PM_pitch = snd.to_pitch(pitch_floor = pitchLo, pitch_ceiling=pitchHi)\n",
    "\n",
    "pitch_tVec = PM_pitch.ts()\n",
    "pitchValues = np.zeros_like(pitch_tVec)\n",
    "\n",
    "for timeIdx, time in enumerate(pitch_tVec):\n",
    "    pitchValues[timeIdx] = PM_pitch.get_value_at_time(time=time)\n",
    "\n",
    "speech_spectro, speech_spectro_t, speech_spectr_f = PM_get_spectrogram(snd, 30, maximumFrequency=4000)\n",
    "\n",
    "windowLength = 30 # ms\n",
    "maxNumberFormants = 4\n",
    "maxFormantFreq = 4000\n",
    "\n",
    "PM_formants = snd.to_formant_burg(maximum_formant=maxFormantFreq, window_length=windowLength/1000,\n",
    "                                  max_number_of_formants=maxNumberFormants)\n",
    "\n",
    "formant_tVec = PM_formants.ts()\n",
    "formantValues = np.zeros((maxNumberFormants,formant_tVec.size))\n",
    "\n",
    "for timeIdx, time in enumerate(formant_tVec):\n",
    "    for formantIdx in range(maxNumberFormants):\n",
    "        formantValues[formantIdx,timeIdx] = PM_formants.get_value_at_time(formant_number=formantIdx+1, time=time)\n",
    "\n",
    "plottitle = 'Spectrogram, f0 and Formants of recorded Sample'\n",
    "SCplot.plot_spectrogram_with_formants(speech_spectro, speech_spectro_t, speech_spectr_f, formantValues, formant_tVec,plottitle,\n",
    "                                      pitchValues=pitchValues, pitch_tVec=pitch_tVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot: Recording of Dada-Sentence\n",
    "Now, go back to the \"record\" button and record the sentence with the same voice melody, but just say \"DaDa\".\n",
    ">-time-domain signal plot\n",
    "\n",
    ">-Spectrogram/Formant/f0 plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = deleteNan(indata)\n",
    "#convert to mono\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "#plot time-domain signal\n",
    "SCplot.get_plot_time_domain_sig(recData,rec_time,'Recorded Time-Domain-Signal',showPlot=True)\n",
    "\n",
    "\n",
    "# Analyze Formants and f0 with parselmouth:\n",
    "pitchLo = 75 #Hz\n",
    "pitchHi = 400 #Hz\n",
    "pitchTimeStep = 30 #ms\n",
    "\n",
    "snd = parselmouth.Sound(recData)\n",
    "\n",
    "PM_pitch = snd.to_pitch(pitch_floor = pitchLo, pitch_ceiling=pitchHi)\n",
    "\n",
    "pitch_tVec = PM_pitch.ts()\n",
    "pitchValues = np.zeros_like(pitch_tVec)\n",
    "\n",
    "\n",
    "for timeIdx, time in enumerate(pitch_tVec):\n",
    "    pitchValues[timeIdx] = PM_pitch.get_value_at_time(time=time)\n",
    "\n",
    "speech_spectro, speech_spectro_t, speech_spectr_f = PM_get_spectrogram(snd, 30, maximumFrequency=4000)\n",
    "\n",
    "\n",
    "windowLength = 30 # ms\n",
    "maxNumberFormants = 4\n",
    "maxFormantFreq = 4000\n",
    "\n",
    "PM_formants = snd.to_formant_burg(maximum_formant=maxFormantFreq, window_length=windowLength/1000,\n",
    "                                  max_number_of_formants=maxNumberFormants)\n",
    "\n",
    "formant_tVec = PM_formants.ts()\n",
    "formantValues = np.zeros((maxNumberFormants,formant_tVec.size))\n",
    "\n",
    "\n",
    "for timeIdx, time in enumerate(formant_tVec):\n",
    "    for formantIdx in range(maxNumberFormants):\n",
    "        formantValues[formantIdx,timeIdx] = PM_formants.get_value_at_time(formant_number=formantIdx+1, time=time)\n",
    "\n",
    "plottitle = 'Spectrogram, f0 and Formants of recorded Sample'\n",
    "SCplot.plot_spectrogram_with_formants(speech_spectro, speech_spectro_t, speech_spectr_f, formantValues, formant_tVec,plottitle,\n",
    "                                      pitchValues=pitchValues, pitch_tVec=pitch_tVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot: Recording of whispered Sentence\n",
    "Now, go back to the \"record\" button and record the sentence with the same voice melody, but whisper only.\n",
    "\n",
    ">-time-domain signal plot\n",
    "\n",
    ">-Spectrogram/Formant/f0 plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = deleteNan(indata)\n",
    "#convert to mono\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "\n",
    "#plot time-domain signal \n",
    "SCplot.get_plot_time_domain_sig(recData,rec_time,'Recorded Time-Domain-Signal',showPlot=True)\n",
    "\n",
    "# Analyze Formants and f0 with parselmouth:\n",
    "pitchLo = 75 #Hz\n",
    "pitchHi = 400 #Hz\n",
    "pitchTimeStep = 30 #ms\n",
    "\n",
    "snd = parselmouth.Sound(recData)\n",
    "\n",
    "PM_pitch = snd.to_pitch(pitch_floor = pitchLo, pitch_ceiling=pitchHi)\n",
    "\n",
    "pitch_tVec = PM_pitch.ts()\n",
    "pitchValues = np.zeros_like(pitch_tVec)\n",
    "\n",
    "\n",
    "for timeIdx, time in enumerate(pitch_tVec):\n",
    "    pitchValues[timeIdx] = PM_pitch.get_value_at_time(time=time)\n",
    "\n",
    "speech_spectro, speech_spectro_t, speech_spectr_f = PM_get_spectrogram(snd, 30, maximumFrequency=4000)\n",
    "\n",
    "\n",
    "windowLength = 30 # ms\n",
    "maxNumberFormants = 4\n",
    "maxFormantFreq = 4000\n",
    "\n",
    "PM_formants = snd.to_formant_burg(maximum_formant=maxFormantFreq, window_length=windowLength/1000,\n",
    "                                  max_number_of_formants=maxNumberFormants)\n",
    "\n",
    "formant_tVec = PM_formants.ts()\n",
    "formantValues = np.zeros((maxNumberFormants,formant_tVec.size))\n",
    "\n",
    "for timeIdx, time in enumerate(formant_tVec):\n",
    "    for formantIdx in range(maxNumberFormants):\n",
    "        formantValues[formantIdx,timeIdx] = PM_formants.get_value_at_time(formant_number=formantIdx+1, time=time)\n",
    "\n",
    "plottitle = 'Spectrogram, f0 and Formants of recorded Sample'\n",
    "SCplot.plot_spectrogram_with_formants(speech_spectro, speech_spectro_t, speech_spectr_f, formantValues, formant_tVec,plottitle,\n",
    "                                      pitchValues=pitchValues, pitch_tVec=pitch_tVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
