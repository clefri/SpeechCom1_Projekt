{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Communication Laboratory: Speech Analysis\n",
    "This Jupyter Notebook is used for the Lesson **Speech Analysis** in the Speech Communication Laboratory. It provides functionality from **PRAAT** in Python using the Library **praat-parselmouth**.\n",
    "* PRAAT documentation: <http://www.fon.hum.uva.nl/praat/>\n",
    "* Parselmouth documentation: <https://parselmouth.readthedocs.io/en/stable/>\n",
    "\n",
    "---\n",
    "### For the Lab Teacher: To Do before using this notebook (for first-time users)\n",
    "* All Python-packages that were used in the Python-environment when this Notebook was created are listed in the Conda environment-file **environment_SC1.yml** or the requirements-file **requirements.txt**. These files can be used to install the necessary packages.\n",
    "* **This notebook is intended for the Lab Teacher/Lab Assistant.** It contains all answers and the code is completed. The notebook for the students can be derived from this notebook, when you delete the answers and those parts of the code that the students should fill in. It is intended that the notebook for the students is created by the Lab Teacher/Lab Assistant.\n",
    "* Some functionality that is not important for the Speech Signal Processing tasks is implemented in the file **Helper_functions.py** (namespace SCtools) and the plots are created using the functions from **Plot_functions.py** (namespace SCplot).\n",
    "---\n",
    "\n",
    "### Structure of the Notebook\n",
    "This notebook includes four experiments and one additional bonus experiment.\n",
    "1. **Experiment 1:** Time-Domain Analysis\n",
    "1. **Experiment 2:** Frequency-Domain Analysis\n",
    "1. **Experiment 3:** Estimation of the Spectral Envelope using Cepstrum and LPC\n",
    "1. **Experiment 4:** Formant structure Analysis\n",
    "1. **(Bonus) Experiment 5:** Record your own formant sample\n",
    "\n",
    "\n",
    "### Lab Protocol = Export of the notebook\n",
    "You should write your answers/remarks/findings/discussions/etc directly into the notebook. To do this, you can create **Markdown**-cells, which have some layout and formatting capabilities. At the end of the lab session, you should export the notebook to PDF, using **File --> Print Preview** and print the Print Preview with the PDF-printer. Please hand in the PDF-Export at the end of the lab lesson.\n",
    "\n",
    "### Export of interactive figures\n",
    "To export the interactive figures correctly, you need to use the variables **exportValue** or **exportRange**. In the cell of the interactive figure, please set this variable to the desired value of the slider (e.g. time instance or time range) and re-run the cell. Otherwise the slider-settings of your plot are not exported.  \n",
    "The values currently stored in **exportValue** or **exportRange** need to be modified to get meaningful plots!\n",
    "\n",
    "---\n",
    "\n",
    "### Members of the Lab group - *please edit*\n",
    "*List the team members here*\n",
    "\n",
    "|First Name |Last Name  |Matr.Nr.   |\n",
    "|:----------|:----------|:----------|\n",
    "|Alan V.    |Oppenheim  |012345678  |\n",
    "|Ronald W.  |Schafer    |012345678  |\n",
    "|Otto       |Toeplitz   |012345678  |\n",
    "\n",
    "### Metadata - *please edit*\n",
    "\n",
    "|Date of Lab Lesson |XX/XX/2020     |\n",
    "|:------------------|:--------------|\n",
    "|Lab Teacher        |Leonhard Euler |\n",
    "|Lab Assistant      |Rudolf Kalman  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction: Load Modules and Import Audio\n",
    "\n",
    "### Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# uncomment the following 2 lines to reload the modules automatically,\n",
    "# such that changes to Plot_functions.py and Helper_functions.py are reloaded without\n",
    "# restarting the kernel!\n",
    "#\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "#\n",
    "###########################################################################################\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "import soundfile as sf\n",
    "import bokeh\n",
    "import sounddevice as sd\n",
    "import time as clock\n",
    "import Plot_functions as SCplot # imports the necessary plot functions\n",
    "import Helper_functions as SCtools # imports the necessary helper functions\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, ifft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Playback Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Choose an Audio File\n",
    "fileName = 'f116.wav'\n",
    "#fileName = 'f216.wav'\n",
    "#fileName = 'a_8000.wav'\n",
    "#fileName = '1000hz_3sec.wav'\n",
    "\n",
    "snd, audio1, fs, filePath = SCtools.import_sound_file(fileName)\n",
    "ipd.Audio(filePath) # show audio player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Experiment 1: Time-Domain Analysis\n",
    "## Short-Time Average Energy (Intensity)\n",
    "\n",
    "In this experiment we will discuss the intensity curve in context with speech analysis and investigate the effects of different window lengths. To calculate the intensity curve, we firstly use our own function based on pythons 'signal' module. After that, we will move on to the methods provided by the library 'praat-parselmouth', which provides the functionality of the software Praat in Python using the original Praat functions, which are written in C++.\n",
    "> Praat documentation: <http://www.fon.hum.uva.nl/praat/manual/Intro.html>\n",
    "\n",
    "\n",
    "### Own Implementation using pythons 'signal' library\n",
    "To calculate the short-time average intensity, we implement the function SC_intensity(). In this function, the signal gets averaged using a Gaussian window. A Gaussian window requires 2 parameters, see [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.gaussian.html#scipy.signal.windows.gaussian), which are the *window length in samples* and the *standard deviation*. We use the window length in seconds as an input parameter, but for the standard deviation we are not sure *a priori*. Unfortunately also Praat's documentation is not very helpful here, as it provides no information about the standard deviation. Therefore, we use [Matlab's documentation of the Gaussian window](https://de.mathworks.com/help/signal/ref/gausswin.html) where we seek help to find a value for the standard deviation, for which we trust Matlab's default value of alpha=2.5.\n",
    "\n",
    "### Task 1.1: Find a suitable value for the standard deviation with the help of Matlab's documentation of the Gaussian window.\n",
    "#### Expected Answer:\n",
    "- the student should recognize from Matlabs documentation that a reasonable value for the standard deviation (or std in the code below) is given as sigma = (L-1)/(2*alpha) with alpha=2.5, where L is the length of the window in samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "windowLength = 20 #millisecond\n",
    "\n",
    "#Calculate minimum pitch from window length\n",
    "minimumPitch = 1000/windowLength\n",
    "\n",
    "def SC_intensity(sound, minimumPitch, fs):\n",
    "    #winLenEffective = np.round(3.2/minimumPitch * fs)  # Window length in samples; from Praat documentation; for pitch-synchronous intensity ripple\n",
    "    winLenEffective = np.round(1/minimumPitch * fs)\n",
    "    \n",
    "    # TODO: find a suitable value for the standard deviation std\n",
    "    # std = ?\n",
    "    alpha = 2.5  # default value from Matlab documentation\n",
    "    std = (winLenEffective-1)/(2*alpha)  # standard deviation from Matlab documentation\n",
    "    # TODO_END\n",
    "    win = signal.windows.gaussian(winLenEffective, std) # Gaussian window\n",
    "    #win = signal.windows.kaiser(winLenEffective, 20/np.pi) # Kaiser window - alternative to Gaussian window\n",
    "    \n",
    "    sound = np.square(sound-np.mean(sound)) \n",
    "\n",
    "    intensity = np.convolve(sound, win, mode='valid') /np.sum(win)\n",
    "    \n",
    "    intensity = 10*np.log10(intensity/(4e-10)) # conversion to dB ; norm to (20 muPa)^2\n",
    "\n",
    "    print(\"SC_intensity: Intensity has {} samples\".format(intensity.size))\n",
    "    winLenEffectiveTime = winLenEffective / fs\n",
    "    return intensity, win, winLenEffectiveTime\n",
    "\n",
    "SC_intensity, gaussWin, winLenEffectiveTime = SC_intensity(np.squeeze(snd.values), minimumPitch, fs)\n",
    "\n",
    "# Plot window function\n",
    "plottitle = \"Gaussian Window for Averaging with SC_intensity()\"\n",
    "dt_win = np.arange(0, gaussWin.size) / fs  # time axis for Gaussian Window\n",
    "p_window = SCplot.get_plot_window(gaussWin, dt_win, plottitle)\n",
    "\n",
    "# Plot intensity curve\n",
    "snd_values = np.squeeze(snd.values)\n",
    "dt_snd = np.arange(0, snd_values.size) / fs\n",
    "dt_SC_intensity = np.arange(0,SC_intensity.size) / fs \n",
    "dt_SC_intensity = dt_SC_intensity + winLenEffectiveTime/2 # shift to center the intensity curve bins in the windows\n",
    "\n",
    "plottitle = 'Intensity calculated with SC_intensity() - File: ' + fileName\n",
    "p_intensity = SCplot.get_plot_intensity(snd_values, dt_snd, SC_intensity, dt_SC_intensity, plottitle)\n",
    "\n",
    "SCplot.plot_in_subplots(p_window, p_intensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Praat-Parselmouth implementation\n",
    "Also, the library praat-parselmouth provides us with functionality to calculate the intensity. To calculate the intensity with parselmouth, the member function to_intensity() is used, which takes the minimum pitch as an input argument. To compare parselmouth's to_intensity() with our custom SC_intensity(), we use the same input parameters as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PM_intensity = snd.to_intensity(minimum_pitch=minimumPitch, subtract_mean=False)  # intensity calculation with parselmouth's function\n",
    "PM_intensity = snd.to_intensity(minimum_pitch=minimumPitch, subtract_mean=False, time_step=1/fs)  # intensity calculation with parselmouth's function\n",
    "print(\"PM_intensity: Intensity has {} samples\".format(PM_intensity.get_number_of_frames()))\n",
    "\n",
    "dt_PM_intensity = PM_intensity.x_grid()[:-1]\n",
    "PM_intensity_val = np.squeeze(PM_intensity.values)\n",
    "dt_snd = snd.x_grid()[:-1]\n",
    "\n",
    "plottitle = \"Intensity calculated with parselmouth's to_intensity() - File: \" + fileName\n",
    "SCplot.get_plot_intensity(np.squeeze(snd.values), dt_snd, PM_intensity_val, dt_PM_intensity, plottitle, showPlot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the two intensity curves\n",
    "Compare the two intensity curves in the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 2 intensity curves in one plot\n",
    "plottitle = \"Comparison of both intensity curves - File: \" + fileName\n",
    "SCplot.plot_two_intensity_curves(dt_PM_intensity, PM_intensity_val, dt_SC_intensity, SC_intensity, plottitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Describe the key differences of the intensity curve calculated with your own function SC_intensity() to parselmouth's to_intensity().\n",
    "#### Expected Answers:\n",
    "- parselmouths intensity is more sparsely sampled\n",
    "- parselmouths intensity has not the same time range as the audio file, whereas the custom intensity covers the whole time range of the audio file\n",
    "\n",
    "### Task 1.3: What happens to the intensity curve, if we use different window sizes?\n",
    "#### Expected answers:\n",
    "- short windows allow more fluctuations of the intensity curve\n",
    "- long windows lead to more smoothing of the intensity curve, thus the fluctuations of the intensity curve will become smaller\n",
    "\n",
    "\n",
    "### Task 1.4: Is there a way to alter the intensity curve calculated with your own function SC_intensity(), such that it matches the intensity curve calculated with parselmouth better?\n",
    "> Hint: see <http://www.fon.hum.uva.nl/praat/manual/Sound__To_Intensity___.html>\n",
    "#### Expected Answer:\n",
    "- the student should recognize that parselmouth uses an effective window length that is 3.2 times the original window length\n",
    "- the student should modify the function SC_intensity() such that it uses this effective window length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Experiment 2: Frequency-Domain Analysis\n",
    "In this experiment, we analyze the sound file in frequency domain, for example using spectrograms. Again, we implement a spectrogram using Python's 'signal' model, and compare it with Praat-Parselmouth's spectrogram.\n",
    "### Load and Playback Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an Audio File\n",
    "#fileName = 'f116.wav'\n",
    "#fileName = 'f216.wav'\n",
    "fileName = 'a_8000.wav'\n",
    "#fileName = '1000hz_3sec.wav'\n",
    "\n",
    "snd, audio1, fs, filePath = SCtools.import_sound_file(fileName)\n",
    "ipd.Audio(filePath) # show audio player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation using Python's 'signal' module\n",
    "First, we start by calculating a spectrogram using the method scipy.signal.spectrogram()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "windowlengthSec = 30 #ms\n",
    "windowlength = np.round(fs * windowlengthSec/1000).astype(int)\n",
    "#windowlength = 2048\n",
    "print('Window Length in samples:', windowlength)\n",
    "#overlap = windowlength-1\n",
    "overlap = np.round(windowlength / 2)\n",
    "\n",
    "# Gaussian window\n",
    "std = (windowlength - 1)/(2*2.5)  # see Task 1.1\n",
    "window = ('gaussian', std) # tuple for signal.spectrogram\n",
    "\n",
    "#window = 'hann' # Hann window - alternative to Gaussian window\n",
    "\n",
    "SC_fVec, SC_tVec, SC_spectroData = signal.spectrogram(audio1, fs=fs, window=window, noverlap=overlap, nperseg=windowlength, return_onesided=True, scaling='spectrum', mode='magnitude')\n",
    "\n",
    "SC_spectroDataDB = 20*np.log10(SC_spectroData / np.max(SC_spectroData))\n",
    "\n",
    "# plot interactive spectrogram\n",
    "\n",
    "# TODO: choose slider Export-Value which produces your desired plot\n",
    "exportValue = 0\n",
    "# TODO_END\n",
    "\n",
    "plottitle = \"Custom Spectrogram of Sound Sample - File: \" + fileName\n",
    "SC_timeWidget = SCplot.plot_interactive_spectrogram(SC_spectroDataDB, SC_tVec, SC_fVec, plottitle,exportValue)\n",
    "widgets.HBox([SC_timeWidget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation using Praat-Parselmouth\n",
    "Now we use parselmouths to_spectrogram() to calculate a spectrogram. For Plotting the spectrogram, we use our custum function plot_interactive_spectrogram()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "windowlengthSec = 70 #ms\n",
    "maximumFrequency = 5000\n",
    "\n",
    "def PM_get_spectrogram(snd, windowLengthMS=30, maximumFrequency=5000):\n",
    "    spectrogram = snd.to_spectrogram(window_length=windowLengthMS/1000, maximum_frequency=maximumFrequency)\n",
    "    PM_spectroData = spectrogram.values\n",
    "    PM_tVec = spectrogram.ts()\n",
    "    PM_fVec = spectrogram.ys()\n",
    "    PM_spectroDataDB = 10*np.log10(PM_spectroData / np.max(PM_spectroData))\n",
    "    return PM_spectroDataDB, PM_tVec, PM_fVec\n",
    "\n",
    "PM_spectroDataDB, PM_tVec, PM_fVec = PM_get_spectrogram(snd, windowLengthMS=windowlengthSec, maximumFrequency=maximumFrequency)\n",
    "\n",
    "# TODO: choose slider Export-Value which produces your desired plot\n",
    "exportValue = 0\n",
    "# TODO_END\n",
    "\n",
    "plottitle = \"Parselmouth Spectrogram of Sound Sample - File: \" + fileName\n",
    "PM_timeWidget = SCplot.plot_interactive_spectrogram(PM_spectroDataDB, PM_tVec, PM_fVec, plottitle, exportValue)\n",
    "widgets.HBox([PM_timeWidget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Effects of different window lengths\n",
    "Modify the window length in the spectrogram generated using Praat-Parselmouth and set it to the following values:\n",
    "- 10 ms\n",
    "- 30 ms\n",
    "- 70 ms\n",
    "\n",
    "Describe the effects you see in the spectrogram and in the spectrum. What could 'wide band analysis' and 'narrow band analysis' mean in this context?\n",
    "#### Expected answers:\n",
    "- the shorter the window, the better is the time resolution, but the frequencfy resolution gets worse\n",
    "- if we use long windows, the frequency resolution gets better, but the time resolution gets worse\n",
    "- 'wide band analysis' refers to spectrogram analysis using short windows, resulting in a good time resolution, but a poor frequency resolution (i.e. the frequencies are only displayed as 'wide bands')\n",
    "- 'narrow band analysis' refers to spectrogram analysis using long windows, thus with a good frequency resolution, but a poor time resolution\n",
    "\n",
    "### Task 2.2: Interpretation of the spectrum and the spectrogram\n",
    "What information can be retrieved from the spectrogram? Think of parameters of the signal that are easily represented in frequancy domain.\n",
    "#### Expected answers:\n",
    "- fundamental frequency f0\n",
    "- harmonic structure of the signal, formant structure\n",
    "- what kind of voice signal is it? (vowel, consonant or fricative)\n",
    "\n",
    "##### Hint:\n",
    "Don't forget to fix the variable \"exportValue\" with a reasonable time-slider-position before you export the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of f0 and formants\n",
    "To analyze the Formants, we use praat-parselmouths formant analysis methods. We analyse the fundamental frequency f0 and additionally the first four formants F1,...,F4.\n",
    "\n",
    "Select sound file to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an Audio File\n",
    "#fileName = 'f116.wav'\n",
    "#fileName = 'f216.wav'\n",
    "#fileName = 'e_11025.wav'\n",
    "fileName = 'a_8000.wav'\n",
    "#fileName = '1000hz_3sec.wav'\n",
    "\n",
    "snd, _, fs, filePath = SCtools.import_sound_file(fileName)\n",
    "ipd.Audio(filePath) # show audio player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the fundamental frequency and the formants, we use the Praat-parselmouth methods to_pitch() for the fundamental frequency and to_formant_burg() for the formants. For the formant analysis, the BURG-algorithm is used (more information about this algorithm, which is based on LPC internally, is [here](https://asa.scitation.org/doi/10.1121/1.2003944))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def PM_get_f0_and_formants(snd, pitchLo = 75, pitchHi = 400, pitchTimeStep = 30, FormantWindowLength = 30,\n",
    "                           maxNumberFormants = 4, maxFormantFreq = 4000):\n",
    "\n",
    "    # evaluate pitch using praat-parselmouth\n",
    "    PM_pitch = snd.to_pitch(pitch_floor = pitchLo, pitch_ceiling=pitchHi)\n",
    "\n",
    "    pitch_tVec = PM_pitch.ts()\n",
    "    pitchValues = np.zeros_like(pitch_tVec)\n",
    "\n",
    "    for timeIdx, time in enumerate(pitch_tVec):\n",
    "        pitchValues[timeIdx] = PM_pitch.get_value_at_time(time=time)\n",
    "\n",
    "    # evaluate formants using praat-parselmouth (here: BURG-algorithm is used)\n",
    "    PM_formants = snd.to_formant_burg(maximum_formant=maxFormantFreq, window_length=FormantWindowLength/1000,\n",
    "                                      max_number_of_formants=maxNumberFormants)\n",
    "\n",
    "    formant_tVec = PM_formants.ts()\n",
    "    formantValues = np.zeros((maxNumberFormants,formant_tVec.size))\n",
    "\n",
    "    for timeIdx, time in enumerate(formant_tVec):\n",
    "        for formantIdx in range(maxNumberFormants):\n",
    "            formantValues[formantIdx,timeIdx] = PM_formants.get_value_at_time(formant_number=formantIdx+1, time=time)\n",
    "    \n",
    "    return pitchValues, pitch_tVec, formantValues, formant_tVec\n",
    "\n",
    "# calculate spectrogram using praat-parselmouth\n",
    "spectroDataDB, spec_tVec, spec_fVec = PM_get_spectrogram(snd, 30, maximumFrequency)\n",
    "\n",
    "# calculate f0 and formants using praat-parselmouth\n",
    "pitchValues, pitch_tVec, formantValues, formant_tVec = PM_get_f0_and_formants(snd)\n",
    "\n",
    "# plot spectrogram with f0 and formants\n",
    "plottitle = \"Spectrogram, f0 and Formants of Sound Sample - File: \" + fileName\n",
    "SCplot.plot_spectrogram_with_formants(spectroDataDB, spec_tVec, spec_fVec, formantValues, formant_tVec,plottitle,\n",
    "                                      pitchValues=pitchValues, pitch_tVec=pitch_tVec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formants F1 & F2 in the Vowel chart\n",
    "Especially with vowels, the first two formants F1 and F2 are very important in order to distinguish one vowel from another. Therefore, F1 and F2 can be displayed in the so-called vowel chart. Below is an interactive vowel chart where you can select a region in the audio file from which the detected formants are displayed in the vowel chart.\n",
    "The vowel chart used here is by Sendlmeier et. al - for more information click [here](https://www.kw.tu-berlin.de/fileadmin/a01311100/Formantkarten_des_deutschen_Vokalsystems_01.pdf).\n",
    "\n",
    "##### Hint:\n",
    "Don't forget to fix the variable \"exportRange\" with a reasonable time-range before you export the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dt_snd = snd.x_grid()[:-1]\n",
    "F1 = formantValues[0,:]\n",
    "F2 = formantValues[1,:]\n",
    "\n",
    "# TODO: choose slider Export-Value which produces your desired plot\n",
    "exportRange = np.array([formant_tVec[0], formant_tVec[-1]])\n",
    "# exportRange = np.array([0 , 1]) # time range in sec\n",
    "# TODO_END\n",
    "\n",
    "plottitle = \"Detected Formants F1 & F2 in the Vowel Chart - File: \" + fileName\n",
    "PM_timeWidget = SCplot.plot_F1_F2_in_vowel_chart(np.squeeze(snd.values), dt_snd, F1, F2, formant_tVec, plottitle, exportRange)\n",
    "widgets.HBox([PM_timeWidget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Experiment 3: Estimation of the Spectral Envelope using Cepstrum and LPC\n",
    "## LPC: Levinson Durbin Algorithm\n",
    "Speech signals are often modelled as a source-filter model. In order to seperate source and filter, the filter has to be estimated. To estimate those filter coefficients the LPC is a reliable and common tool - for more information click [here](https://ccrma.stanford.edu/~hskim08/lpc/).\n",
    "\n",
    "The Levinson Durbin-Algorithm is an elegant procedure to calculate the LPC-coefficients recursively. This implementation uses the whole signal for processing. With the help of the LPC-coefficients an All-Pole Filter can be modelled whose frequency response can be understood as the spectral envelope of the chosen signal, if reasonable LPC parameters are chosen.\n",
    "\n",
    "### Task 3.1: LPC Analysis:\n",
    "- If the whole signal is used for LPC analysis, which aspects has the soundfile to fulfill? \n",
    "- How can sentences be analysed?\n",
    "\n",
    "#### Expected Answers:\n",
    "- Estimating the Spectral Envelope of speech signals (more generally: seperating the source signal from the transfer path) requires a stationary signal. If the transfer path changes inside the signal to analyse, the results are not representative. \n",
    "- Therefore, sentences have to be buffered in order to be analysed!\n",
    "\n",
    "Choose a suitable sound file for the Levinson-Durbin-Algorithm from the code lines below, or choose another sample from the file directory. You can listen to a file by uncommenting the related line and running the cell in the code below. \n",
    "\n",
    "##### Keep in mind that the whole signal is used for this particular implementation of the LPC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose an audio file\n",
    "\n",
    "#fileName = 'f116.wav'\n",
    "#fileName = 'f216.wav'\n",
    "fileName = 'a_8000.wav'\n",
    "#fileName = '1000hz_3sec.wav'\n",
    "\n",
    "_, sig, fs, filePath = SCtools.import_sound_file(fileName)\n",
    "ipd.Audio(filePath) # show audio player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set the LPC order below (\"lpcOrder\"), and run the code. The auto-correlation, the spectral-envelope and the Z-plane indicating the modelled All-Pole Filter's poles, are displayed. If there are too many plotted coefficients the results might be confusing. So choose a suitalbe number of coefficients to display the spectral envelope (use the variable \"coefficientBoundary\"). Run the code again and take a look at the results!\n",
    "\n",
    "Take a look at the function \"autocorr()\" and check how the auto-correlation is calculated. The documentation of \"np.correlate()\" can be found [here](https://numpy.org/doc/stable/reference/generated/numpy.correlate.html).\n",
    "\n",
    "The grey line displayed in back of the \"Spectrum and Spectral Envelope\" - plot is the power spectral density (PSD). The power spectral density is defined as the frequency spectrum of the auto-correlation function.\n",
    "\n",
    "### Task 3.2: Pre-Emphasis Filter\n",
    "A pre-emphasis filter can significantly improve the result of a LPC analysis. Use the predefined function \"pre_emphasis_filtering(sig, fs)\" to implement the pre-emphasis filtering before calculating the LPC coefficients.\n",
    "\n",
    "- Implement the pre-emphasis filter and compare the results.\n",
    "- What does a pre-emphasis filter do, and why does the algorithm perform better/worse?\n",
    "\n",
    "#### Expected Answers:\n",
    "- Comparing the results, there is a significant drop to high frequencies in the vocal tract spectrum of the estimation without pre-emphasis. Also the autocorrelation of the pre-emphasis filtered signal drops to much smaller values (with pre-emph.). The results of the estimation with pre-emphasis seem more reliable.\n",
    "- The pre-emphasis filter whitens the spectrum and therefore decorrelates the signal (for time-lags bigger than 0). The Levinson-Durbin Algorithm is constructed to perform best with decorrelated signals. In case of speech, the main signal energy is located in lower frequencies. Therefore the pre-emphasis filter is a low-cut filter, in this case a first order low-cut filter. \n",
    "\n",
    "\n",
    "In the resulting plot below, the iterations of the Levinson-Durbin algorithm can be stepped through. The approach of the coefficients can be observed.\n",
    "\n",
    "### Task 3.3: High Order LPC\n",
    "Choose a very high LPC order, in order to produce peaks which are no longer usual for a spectral envelope.\n",
    "\n",
    "- How can you explain the prominent peaks in the estimated spectral envelope for high LPC orders? What role does the spectral density play in this case?\n",
    "\n",
    "#### Expected Answer:\n",
    "- These peaks represent the harmonics of the speech signal. For too large numbers of coefficients the estimation includes harmonics.  Ultimately it is the LPC's aim to approximate the PSD and therefore the harmonics occur for high LPC orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_spectrum(signalData, NFFT, window = True, logarithm = True):\n",
    "# Calclate Spectrum:\n",
    "# input:\n",
    "#   signalData...input signal\n",
    "#   NFFT...fft order\n",
    "#   window...(bool) window signal with hann window (default True)\n",
    "#   logarithm...(bool) returns the spectrum in dB if true (default True)\n",
    "#\n",
    "# return:\n",
    "#   spetrum...positive frequencies of spectrum\n",
    "#\n",
    "    dataVec = np.zeros(NFFT)\n",
    "    if len(signalData) > NFFT:\n",
    "        dataVec = signalData[0:NFFT-1]\n",
    "        win = signal.hann(NFFT)\n",
    "    else:\n",
    "        dataVec[0:len(signalData)] = signalData\n",
    "        win = signal.hann(len(signalData))\n",
    "        win = np.append(win, np.zeros(NFFT-len(signalData)))\n",
    "    if window:\n",
    "        dataVec = dataVec*win\n",
    "    if logarithm:\n",
    "        spectrum = 20*np.log10(abs(fft(dataVec)))\n",
    "    else:\n",
    "        spectrum = abs(fft(dataVec))\n",
    "    return spectrum[0:round((len(spectrum)+1)/2)]\n",
    "\n",
    "def autocorr(x, norm=True):\n",
    "# Calculate Autocorrelation:\n",
    "# input:\n",
    "#   x...input signal\n",
    "#   norm...(bool) normalize autocorrelation (default True)\n",
    "#\n",
    "# return:\n",
    "#   result...autocorrelation of x (only first half)\n",
    "#\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    result = result[result.size // 2:]\n",
    "    if norm:\n",
    "        result = result/result[0]\n",
    "    return result\n",
    "\n",
    "def pre_emphasis_filtering(sig, fs, inv=False):\n",
    "# Pre-Emphasis Filtering:\n",
    "# input:\n",
    "#   sig...signal to filter\n",
    "#   fs...sampling frequency\n",
    "#   inv...(bool) inversely filtering (default False)\n",
    "#\n",
    "# return:\n",
    "#   sigPreEmphasis...filtered signal\n",
    "#\n",
    "    fPreEmph = 10\n",
    "    # alpha is calculated as in egifa!\n",
    "    alpha = np.exp(-2*np.pi*fPreEmph/fs)\n",
    "    if inv:\n",
    "        # 1. order Low-Cut\n",
    "        sigPreEmphasis = signal.lfilter([1],np.append([1], -alpha),sig)\n",
    "    else:\n",
    "        sigPreEmphasis = signal.lfilter(np.append([1], -alpha),[1],sig)\n",
    "        \n",
    "    return sigPreEmphasis\n",
    "\n",
    "###########################################################################################################################\n",
    "\n",
    "# TODO: set the LPC order and if necessary limit the number of coefficients:\n",
    "lpcOrder = 100\n",
    "coefficientBoundary = lpcOrder\n",
    "# TODO_END\n",
    "\n",
    "E = np.zeros(lpcOrder-1) # define size of error vector\n",
    "K = np.zeros(lpcOrder-1) # define size of reflection coefficient vector\n",
    "a_all = np.zeros([lpcOrder-1, lpcOrder]) # define size of filter coefficient matrix (coefficient vector each row)\n",
    "\n",
    "# TODO: implement pre-emphasis filtering\n",
    "sigPreEmphasis = pre_emphasis_filtering(sig, fs) # pre-emphasis filtering to whiten the speech signal before analyzing it\n",
    "\n",
    "R = autocorr(sigPreEmphasis) # calc. the autocorrelation of the signal (only significant part)\n",
    "#R = autocorr(sig) # calc. the autocorrelation of the signal (only significant part)\n",
    "# TODO_END\n",
    "\n",
    "SCplot.plot_autocorr(R, 'Auto-Correlation') # plot auto-correlation\n",
    "\n",
    "K[0] = R[1]/R[0] # initialize reflection coefficient\n",
    "a = K[0] # initialize lpc coefficient\n",
    "a_all[0,0] = 1\n",
    "a_all[0,1] = -a # save initial lpc coefficient as IIR filter coefficient\n",
    "E[0] = R[0] # initialize error\n",
    "\n",
    "for i in range(1,lpcOrder-1):\n",
    "    \n",
    "    K[i] = (R[i+1] - np.dot(a, R[1:i+1])) / E[i-1] # calc new reflection coefficient\n",
    "    \n",
    "    a = np.append(K[i], a-K[i]*np.flip(a)) # calc new lpc coefficients\n",
    "    \n",
    "    E[i] = E[i-1] * (1-(abs(K[i])**2)) # calc new error\n",
    "    \n",
    "    a_all[i,0:i+2] = np.append([1], -np.flip(a)) # save current lpc coefficients as IIR filter coefficients\n",
    "\n",
    "    \n",
    "# TODO: choose slider Export-Value which produces your desired plot\n",
    "exportValue = 1 # iteration number\n",
    "# TODO_END\n",
    "\n",
    "plotTitle = 'Spectrum and Spectral Envelope'    \n",
    "spectDensity = get_spectrum(np.append(autocorr(sigPreEmphasis, norm=False), np.flip(autocorr(sigPreEmphasis, norm=False)[0:-1])), 2*len(R)-1)\n",
    "SC_iterationWidget = SCplot.plot_interactive_filter_zplane(a_all, coefficientBoundary, fs, spectDensity, plotTitle,exportValue)\n",
    "widgets.HBox([SC_iterationWidget])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the spectral envelope the signal can be filtered with the inverse All-pole filter holding the estimated LPC coefficients. In the cell below the time signals and spectra of the original signal and the inversly-filtered signal are displayed. The estimated spectral envelope can also be interpreted as the transfer-path of the source-filter modell which holds the vocal-tract informatio (formants).\n",
    "\n",
    "Use the interactive plot above to select a LPC order (number of iterations) which results in a reasonable spectral envelope. Don't forget to fix the variable \"exportValue\" with the found LPC order. \n",
    "\n",
    "After a LPC order is found fix the parameters \"selectedIteration\" and \"limitCoefficients\" in the cell below. Typically the number of coefficients coincides with the LPC order. In order to do so, the number of coefficients doesn't have to be limited (LPC order equals number of coefficients if \"limitCoefficients = 0\"). The following figures then correspond to the selected filter.\n",
    "\n",
    "### Task 3.4: Fundamental Frequency f0\n",
    "Zoom into the Glottis time signal and measure the period using the crosshairs. Then calculate the frequency of the Glottis signal and compare it to the fundamental frequency derived from the vocal-tract estimation. If so, why do differences occur?\n",
    "\n",
    "#### Expected Answer:\n",
    "- measured from Glottis signal: T = 0.008s    ->    f0 = 125Hz\n",
    "- measured from Vocal Tract: f0 = 140Hz\n",
    "- The measurement from the glottis signal is a local observation in contrast to the global observation of the estimated vocal tract.\n",
    "\n",
    "### Task 3.5: Think of applications for LPC!\n",
    "#### Expected Answer:\n",
    "- vocoder\n",
    "- telephone (speech coding)\n",
    "- speech analysis\n",
    "- speech synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: select reasonalbe filter coefficients from the interactive plot\n",
    "selectedIteration = 30\n",
    "# TODO_END\n",
    "\n",
    "# TODO: limit number of filter coefficients (if 0, no limit an number of coefficients equals number of iterations)\n",
    "limitCoefficients = 0\n",
    "# TODO_END\n",
    "\n",
    "selectedFilter = np.trim_zeros(a_all[selectedIteration], 'b')\n",
    "if limitCoefficients < len(selectedFilter) and limitCoefficients > 0:\n",
    "    selectedFilter = selectedFilter[0:limitCoefficients]\n",
    "\n",
    "#print(selectedFilter)\n",
    "\n",
    "filteredSignal = signal.lfilter([1], selectedFilter, sig)\n",
    "filteredSignal = filteredSignal / max(abs(filteredSignal)) # normalize\n",
    "inverseFilteredSignal = signal.lfilter(selectedFilter, [1], sig)\n",
    "inverseFilteredSignal = inverseFilteredSignal / max(abs(inverseFilteredSignal)) # normalize\n",
    "inverseFilteredSignalPreEmph = pre_emphasis_filtering(inverseFilteredSignal, fs) # preEmph filtered to lose the speech characteristec level drop to high frequencies\n",
    "inverseFilteredSignalPreEmph = inverseFilteredSignalPreEmph / max(abs(inverseFilteredSignalPreEmph)) # normalize\n",
    "\n",
    "SCplot.plot_time_signal(sig, fs, 'Original Time Signal')\n",
    "#SCplot.plot_time_signal(filteredSignal, fs, 'Vocal Tract Time Signal')\n",
    "SCplot.plot_time_signal(inverseFilteredSignal, fs, 'Excitation Signal (not pre emphasis filtered)')\n",
    "\n",
    "sigSpectrum = get_spectrum(sig, len(sig), window=True)\n",
    "filteredSignalSpectrum = get_spectrum(filteredSignal, len(filteredSignal), window=True)\n",
    "inverseFilteredSignalSpectrum = get_spectrum(inverseFilteredSignal, len(inverseFilteredSignal), window=True)\n",
    "\n",
    "freqVector = np.linspace(0, fs/2, round(len(sig)/2+1))\n",
    "SCplot.plot_spectrum(get_spectrum(sig, len(sig), window=True), freqVector, 'Original Signal - Spectrum')\n",
    "SCplot.plot_spectrum(get_spectrum(sigPreEmphasis, len(sig), window=True), freqVector, 'Pre Emphasis Filtered Signal - Spectrum')\n",
    "SCplot.plot_spectrum(get_spectrum(inverseFilteredSignalPreEmph, len(inverseFilteredSignalPreEmph), window=True), freqVector, 'Excitation Signal - Spectrum (pre emphasis filtered)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cepstrum Analysis\n",
    "In this part the source-filter seperation is carried out in the cepstral domain.\n",
    "Therefore the cepstrum of the signal is calculated (for information click [here](http://piotr.majdak.com/alg/VO/sourcefilter.pdf), page 18).\n",
    "The cepstrum shows the slowly changing parts of a frequency-spectrum in the lower quefrency area whereas the fast oscillating parts are located towards higher quefrencies. This means the information on the spectral envelope can be found towards the lower quefrency area. The spectral envelope can therefore be calculated by liftering the cepstrum using a rectangular lifter for low quefrencies. Ultimately we can also derive the spectral fine structure using the inverse rectangular lifter in order to surpress the spectral envelope and then using the high quefrency content to calculate back towards the spectral fine structure.\n",
    "\n",
    "### Task 3.7: Source-Filter-Seperation through Liftering\n",
    "- Choose the lifter length (\"lifterLength\") in samples to produce a reasonable spectral envelope. To do so, test diffrent lifter lenghts and monitor the plots given below.\n",
    "#### Hint: the chosen lifter length also indirectly defines the inverse lifter length used to compute the spectral fine structure.\n",
    "\n",
    "\n",
    "- Compare the spectral envelope estimation using LPC and the Cepstral method and discuss the differences.\n",
    "\n",
    "#### Expected Answer:\n",
    "- A lifter length arround 30 gives a reasonable vocal tract filter estimation. Too large lifter lenghts result in spectra with speech harmonics visible and so a glottis signal spectrum consisting of noise only. Too small lifter lengths give a glottis signal spectrum with some envelope stil visible.\n",
    "- ANSWER MISSING FOR DIFFERENCES BETWEEN LPC AND CEPSTRAL METHOD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate Cepstrum:\n",
    "# input:\n",
    "#   spectrum...spectrum input (no logarithm)\n",
    "#   logBase10...(bool) if true log10 is used, if false the natural log is used (default True)\n",
    "#   mirrorSpectrum...(bool) mirrors spectrum if treu, use for positive frequency input (default False)\n",
    "#\n",
    "# return:\n",
    "#   cepstrum...cepstrum of positive and negative quefrencies\n",
    "#\n",
    "def get_cepstrum(spectrum, logBase10 = True, mirrorSpectrum = False):\n",
    "    if mirrorSpectrum:\n",
    "        spectrum = np.append(spectrum, np.flip(spectrum[0:-1]))\n",
    "    if logBase10:\n",
    "        spectLog = np.log10(abs(spectrum))\n",
    "    else:\n",
    "        spectLog = np.log(abs(spectrum))\n",
    "    #cepstrum = 4* ifft(spectLog)**2\n",
    "    cepstrum = ifft(spectLog)\n",
    "    return cepstrum\n",
    "        \n",
    "###########################################################################################################################\n",
    "\n",
    "# TODO: set length of cepstrum lifter in samples\n",
    "lifterLength = 30\n",
    "# TODO_END\n",
    "\n",
    "#sig = sig - np.mean(sig)\n",
    "\n",
    "sigLength = len(sig)\n",
    "fftLength = sigLength\n",
    "timeVector = np.linspace(0, sigLength/fs, sigLength)\n",
    "freqVector = np.linspace(0, fs/2, round(fftLength/2+1))\n",
    "\n",
    "\n",
    "spectrum = get_spectrum(sig, len(sig), window = True, logarithm=False)\n",
    "\n",
    "cepstrum = get_cepstrum(spectrum, mirrorSpectrum=True, logBase10=True)\n",
    "cepstrumForEnvelope = get_cepstrum(spectrum, mirrorSpectrum=True, logBase10=True)\n",
    "cepstrumForFineStructure = get_cepstrum(spectrum, mirrorSpectrum=True, logBase10=True)\n",
    "SCplot.plot_cepstrum(20*np.log10(abs(cepstrum[0:round(len(cepstrum)/2+1)])), range(len(cepstrum[0:round(len(cepstrum)/2+1)])), lifterLength, 'Signal Cepstrum', lifterLP=True)\n",
    "\n",
    "cepstrumForEnvelope[lifterLength:-lifterLength] = 0 #liftering\n",
    "\n",
    "cepstrumForFineStructure[0:lifterLength] = 0 #liftering\n",
    "cepstrumForFineStructure[-lifterLength:] = 0 #liftering\n",
    "\n",
    "inverseCepstrumForEnvelope = 10**(fft(cepstrumForEnvelope))\n",
    "inverseCepstrumForEnvelope = inverseCepstrumForEnvelope[0:round(len(inverseCepstrumForEnvelope)/2)]\n",
    "\n",
    "SCplot.plot_spectrum(20*np.log10(abs(inverseCepstrumForEnvelope)), freqVector, 'Spectrum Envelope based on Cepstrum')\n",
    "\n",
    "inverseCepstrumForFineStructure = 10**(fft(cepstrumForFineStructure))\n",
    "inverseCepstrumForFineStructure = inverseCepstrumForFineStructure[0:round(len(inverseCepstrumForFineStructure)/2)]\n",
    "\n",
    "SCplot.plot_spectrum(20*np.log10(abs(inverseCepstrumForFineStructure)), freqVector, 'Spectrum Fine Structure based on Cepstrum')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Experiment 4: Formant structure Analysis\n",
    "\n",
    "In this exercise you need to record your own speech-samples. Before recording is possible the input/output sounddevices have to be set. Use the first cell to display all possible sound-devices and select the wanted devices by assigning the sounddevice-indices to the variables 'input_device' and 'output_device'.\n",
    "\n",
    "Then you can use the given record, play to record your own speech-samples. Please make sure to click into the next cell after you recorded a sample, don't run the 'button'-cell twice otherwise your recording will be deleted.\n",
    "\n",
    "For this experiment please record 3 different versions of one sentence and analyze the time-domain signal and its formant and f0 structure (as already done in Experiment 2) and plot the results in separate cells. By plotting the results in separate cells it is possible to always use the same record button and audio-array for different recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 44100 #Hz\n",
    "\n",
    "#show all possible sound-devices\n",
    "sd.query_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set default fs and number of channels\n",
    "sd.default.samplerate = fs\n",
    "num_channels = 2\n",
    "\n",
    "# TODO: select sound-device by choosing ID of the list shown above.\n",
    "input_device = 0\n",
    "output_device = 1\n",
    "# TODO_END\n",
    "\n",
    "sd.default.device = [input_device,output_device]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# record and playback audio\n",
    "toggleRec,togglePlay,out,box_layout,indata = SCtools.get_rec_and_play_button(fs, num_channels, max_duration = 30)\n",
    "\n",
    "widgets.HBox([toggleRec,togglePlay,out],layout=box_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Recording of Standard Sentence:\n",
    "Record a standard sentence and plot the time-domain signal and the spectrogram/formant/f0 plot as you have already done in Experiment 2. For the time-domain signal use the provided function 'get_plot_time_domain_sig()'. The function-arguments are described in the corresponding function's header available in the file 'plot_functions.py'. To analyze the spectrogram/frequency and f0 structure of the recorded sentence you can use the Parselmouth-analysis and the corresponding plot-functions of Experiment 2 ('PM_get_spetrogram()' and 'PM_get_f0_and_formants()').\n",
    "\n",
    "The recorded Data is stored into the array 'recData'. Please plot the results in a separate Cell and answer the following questions:\n",
    "\n",
    "- Are there visible formant contours?\n",
    "- Can you distinguish between vowels, consonants and fricatives?\n",
    "\n",
    "#### Expected Answers:\n",
    "- Yes there should be visible formants assuming a standard sentence containing vowels was recorded.\n",
    "- Yes vowels are distinguishable with a clear formant structure and fricatives don't show a clear formant-structure. For consonants like 'l', 'm' or 'n' there can be visible formant structures because they can also be used in a voiced way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = SCtools.deleteNan(indata)\n",
    "# the recorded signal is converted to a mono signal and stored in recData\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "snd = parselmouth.Sound(recData)\n",
    "\n",
    "# TODO: plot time-domain signal   \n",
    "SCplot.get_plot_time_domain_sig(recData,rec_time,'Time-Domain-Signal of recorded Sample: \"!ENTER RECORDED WORD/SOUND HERE!\"',showPlot=True)\n",
    "# TODO_END\n",
    "\n",
    "# TODO: calculate spectrogram using praat-parselmouth\n",
    "speech_spectro, speech_spectro_t, speech_spectr_f = PM_get_spectrogram(snd, windowLengthMS=30, maximumFrequency=5000)\n",
    "# TODO_END\n",
    "\n",
    "# TODO: calculate f0 and formants using praat-parselmouth\n",
    "pitchValues, pitch_tVec, formantValues, formant_tVec = PM_get_f0_and_formants(snd)\n",
    "# TODO_END\n",
    "\n",
    "# TODO: plot the spectrogram \n",
    "plottitle = 'Spectrogram, f0 and Formants of recorded Sample: \"!ENTER WORD/SOUND/SENTENCE HERE!\"'\n",
    "SCplot.plot_spectrogram_with_formants(speech_spectro, speech_spectro_t, speech_spectr_f, formantValues, formant_tVec,plottitle,\n",
    "                                      pitchValues=pitchValues, pitch_tVec=pitch_tVec)\n",
    "# TODO_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Recording of Dada-Sentence:\n",
    "Now, please go back to the \"record\" button and record a so called Dada-sentence but try to keep the same pitch and pitch course as in the sentence before. The Dada-sentence should not consist of words but rather of sounds like 'da', 'la', 'fa' etc. After recording please plot the time-domain signal and spectrogram/formant/f0-plot in a separate cell.\n",
    "- Is the course of f0 the same as in the Recording before?\n",
    "- How do the formant-structures differ?\n",
    "- Are there any differences visible in the time-domain?\n",
    "\n",
    "#### Expected Answers:\n",
    "- Should be the same if it has been possible to keep the same pitch as the sentence before, if not maybe try again.\n",
    "- If only one sound e.g. 'da' was used the same formant-structure should repeat itself and it should be visible how the formants of the 'a' contained in 'da', emerge from lower frequencies, due to the fact that a consonant is used before the 'a'.\n",
    "- If a voiced sound like 'da' was used to record the sentence the time-domain signal should consist only of voiced-segments and no \"noisy\" segments. \"Voiced\" segments contain more energy than \"noisy\" whereas noisy segments of sounds like 's' or the german 'sch' fluctuate more randomly and the typical noise structure is visible in the time-domain signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = SCtools.deleteNan(indata)\n",
    "# the recorded signal is converted to a mono signal and stored in recData\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "snd = parselmouth.Sound(recData)\n",
    "\n",
    "# TODO: plot time-domain signal   \n",
    "SCplot.get_plot_time_domain_sig(recData,rec_time,'Time-Domain-Signal of recorded Sample: \"!ENTER RECORDED WORD/SOUND HERE!\"',showPlot=True)\n",
    "# TODO_END\n",
    "\n",
    "# TODO: calculate spectrogram using praat-parselmouth\n",
    "speech_spectro, speech_spectro_t, speech_spectr_f = PM_get_spectrogram(snd, windowLengthMS=30, maximumFrequency=5000)\n",
    "# TODO_END\n",
    "\n",
    "# TODO: calculate f0 and formants using praat-parselmouth\n",
    "pitchValues, pitch_tVec, formantValues, formant_tVec = PM_get_f0_and_formants(snd)\n",
    "# TODO_END\n",
    "\n",
    "# TODO: plot the spectrogram \n",
    "plottitle = 'Spectrogram, f0 and Formants of recorded Sample: \"!ENTER WORD/SOUND/SENTENCE HERE!\"'\n",
    "SCplot.plot_spectrogram_with_formants(speech_spectro, speech_spectro_t, speech_spectr_f, formantValues, formant_tVec,plottitle,\n",
    "                                      pitchValues=pitchValues, pitch_tVec=pitch_tVec)\n",
    "# TODO_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.3: Recording of whispered Sentence:\n",
    "Record the previous sentence but use a whispery voice and once more please plot the time-domain signal and the spectrogram/formant/f0-plot in a separate cell.\n",
    "- How does the formant-structure differ in comparison to the first Recording?\n",
    "- How and why does the f0-course differ from the previouse recordings?\n",
    "\n",
    "#### Expected Answers:\n",
    "- Formant-Tracker results might vary more due to less energy in the speech signal. But some sort of Formant-structure should still be visible. \n",
    "- F0-Tracking does not work. Whispery voice does not contain a f0 structure due to the fact that the glottis does not close when speaking with a whispery voice. The glottis closing impulses are no longer present aand the Excitation signal is now noise-like. The missing glottis closing impulses lead to a missing f0 perception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = SCtools.deleteNan(indata)\n",
    "# the recorded signal is converted to a mono signal and stored in recData\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "snd = parselmouth.Sound(recData)\n",
    "\n",
    "# TODO: plot time-domain signal   \n",
    "SCplot.get_plot_time_domain_sig(recData,rec_time,'Time-Domain-Signal of recorded Sample: \"!ENTER RECORDED WORD/SOUND HERE!\"',showPlot=True)\n",
    "# TODO_END\n",
    "\n",
    "# TODO: calculate spectrogram using praat-parselmouth\n",
    "speech_spectro, speech_spectro_t, speech_spectr_f = PM_get_spectrogram(snd, windowLengthMS=30, maximumFrequency=5000)\n",
    "# TODO_END\n",
    "\n",
    "# TODO: calculate f0 and formants using praat-parselmouth\n",
    "pitchValues, pitch_tVec, formantValues, formant_tVec = PM_get_f0_and_formants(snd)\n",
    "# TODO_END\n",
    "\n",
    "# TODO: plot the spectrogram \n",
    "plottitle = 'Spectrogram, f0 and Formants of recorded Sample: \"!ENTER WORD/SOUND/SENTENCE HERE!\"'\n",
    "SCplot.plot_spectrogram_with_formants(speech_spectro, speech_spectro_t, speech_spectr_f, formantValues, formant_tVec,plottitle,\n",
    "                                      pitchValues=pitchValues, pitch_tVec=pitch_tVec)\n",
    "# TODO_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.4: Recording of Explosives and Fricatives:\n",
    "Now try to record explosive or fricative sounds and analyze them.\n",
    "\n",
    "- Are the formants and the f0 structures still visible?\n",
    "- Is it possible to create a voiced fricative? If so please plot the result of a voiced fricative in the exported notebook.\n",
    "\n",
    "#### Expected Answers:\n",
    "- For explosives or pure fricatives no clear formant/f0 structure should be visible or at least the found formants/f0 should show a very noisy behaviour and no stable regions.\n",
    "- Yes there phonems which can be interpreted as voiced fricatives e.g. if an 's' is used before a vowel as in the word \"Susie\".\n",
    "\n",
    "##### Hint:\n",
    "For better analysis result you might want to use a better time resolution (wide band analysis). Use the parameters \"windowLengthMS\" of the Function 'PM_get_spectrogram()' , \"pitchTimeStep\" and \"FormantWindowLength\" of the function 'PM_get_f0_and_formants' to change the window-length. Start with an analysis window of 15 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = SCtools.deleteNan(indata)\n",
    "# the recorded signal is converted to a mono signal and stored in recData\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "snd = parselmouth.Sound(recData)\n",
    "\n",
    "# TODO: choose the analysis-window-length\n",
    "winLen = 15\n",
    "# TODO_END\n",
    "\n",
    "# TODO: plot time-domain signal   \n",
    "SCplot.get_plot_time_domain_sig(recData,rec_time,'Time-Domain-Signal of recorded Sample: \"!ENTER RECORDED WORD/SOUND HERE!\"',showPlot=True)\n",
    "# TODO_END\n",
    "\n",
    "# TODO: calculate spectrogram using praat-parselmouth\n",
    "speech_spectro, speech_spectro_t, speech_spectr_f = PM_get_spectrogram(snd, windowLengthMS=winLen, maximumFrequency=5000)\n",
    "# TODO_END\n",
    "\n",
    "# TODO: calculate f0 and formants using praat-parselmouth\n",
    "pitchValues, pitch_tVec, formantValues, formant_tVec = PM_get_f0_and_formants(snd,pitchLo = 75, pitchHi = 400, pitchTimeStep = winLen, FormantWindowLength = winLen,\n",
    "                           maxNumberFormants = 4, maxFormantFreq = 4000)\n",
    "# TODO_END\n",
    "\n",
    "# TODO: plot the spectrogram \n",
    "plottitle = 'Spectrogram, f0 and Formants of recorded Sample: \"!ENTER WORD/SOUND/SENTENCE HERE!\"'\n",
    "SCplot.plot_spectrogram_with_formants(speech_spectro, speech_spectro_t, speech_spectr_f, formantValues, formant_tVec,plottitle,\n",
    "                                      pitchValues=pitchValues, pitch_tVec=pitch_tVec)\n",
    "# TODO_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# (Bonus) Experiment 5: Record your own formant sample\n",
    "Use the provided 'Record' and 'Play' Button to record your own formant samples and visualize the results with the help of the previously used plot function 'plot_F1_F2_in_vowel_chart()' (see Experiment 2).\n",
    "\n",
    "###### Hint: \n",
    "Make shure to limit the recording towards the voiced parts to avoid wrong formant detections. To do so the time-widget included in 'plot_F1_F2_in_vowel_chart()' can be a very helpful tool. If the slider does not respond whilst dragging it try to run the cell again! Don't forget to store the found time-range in the variable 'exportRange'!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record and playback audio\n",
    "fs = 11025 # lower sampling rate in order to reduce computational effort. \n",
    "sd.default.samplerate = fs\n",
    "toggleRec,togglePlay,out,box_layout,indata = SCtools.get_rec_and_play_button(fs, num_channels, max_duration = 30)\n",
    "\n",
    "widgets.HBox([toggleRec,togglePlay,out],layout=box_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Recording of single vowel:\n",
    "Try to record simple formant samples which contains a single vowel ('a','e','i','o'...).\n",
    "- Do the results match the given F1/F2 chart? Please plot a recorded vowel for which the analysis matches the given chart.\n",
    "\n",
    "#### Expected Answer:\n",
    "- Vowels like 'i' or 'e' have proven themselves to produce good results which match the given chart reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = SCtools.deleteNan(indata)\n",
    "#convert to mono\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "snd = parselmouth.Sound(recData,sampling_frequency=fs)\n",
    "# calculate f0 and formants using praat-parselmouth\n",
    "pitchValues, pitch_tVec, formantValues, formant_tVec = PM_get_f0_and_formants(snd)\n",
    "\n",
    "#  TODO: extract the array containing the time axis and the formants F1 and F2 \n",
    "dt_snd = snd.x_grid()[:-1]\n",
    "F1 = formantValues[0,:]\n",
    "F2 = formantValues[1,:]\n",
    "# TODO_END\n",
    "\n",
    "\n",
    "# TODO: choose slider Export-Value which produces your desired plot\n",
    "exportRange = np.array([formant_tVec[0], formant_tVec[-1]])\n",
    "# exportRange = np.array([0 , 1]) # time range in sec\n",
    "# TODO_END\n",
    "\n",
    "\n",
    "# TODO: Plot the recorded Formant inside the F1/F2 Vowel Chart (see Experiment 2)\n",
    "plottitle = \"Recorded and detected Formants F1 & F2 in the Vowel Chart\"\n",
    "PM_timeWidget = SCplot.plot_F1_F2_in_vowel_chart(np.squeeze(snd.values), dt_snd, F1, F2, formant_tVec, plottitle,exportRange)\n",
    "widgets.HBox([PM_timeWidget])\n",
    "# TODO_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Recording of a diphtong:\n",
    "Try to record a formant samples which contains a diphtong vowel ('ai','au','ei','ou'...).\n",
    "- Again please visualize the tracked formants with the given plot-function in the F1/F2 vowel chart (see Experiment 2). Is the course of the changing formants visible within the vowel-chart?\n",
    "- Why are there more sample points visible where a vowel is held in contrast to the transition region in between the vowels where the sample points are more sparse?\n",
    "\n",
    "#### Expected Answer:\n",
    "- Works very well if time range is set to limit sample towards voiced regions.\n",
    "- The transition between two vowels is obtained by trained muscle movements which humans execute very fast and almost automatically. If the muscle movements are impaired for example due to the influence of alcohol those movements are becoming more slow and the transition doesn't happen that fast. Typically this is percieved as \"Lallen\" (in German)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare recordings for plots\n",
    "indata_no_Nan = SCtools.deleteNan(indata)\n",
    "#convert to mono\n",
    "recData = (indata_no_Nan[:,0]+indata_no_Nan[:,1])/2\n",
    "rec_time = np.linspace(0,recData.shape[0]/fs,recData.shape[0])\n",
    "snd = parselmouth.Sound(recData,sampling_frequency=fs)\n",
    "# calculate f0 and formants using praat-parselmouth\n",
    "pitchValues, pitch_tVec, formantValues, formant_tVec = PM_get_f0_and_formants(snd)\n",
    "\n",
    "#  TODO: extract the array containing the time axis and the formants F1 and F2 \n",
    "dt_snd = snd.x_grid()[:-1]\n",
    "F1 = formantValues[0,:]\n",
    "F2 = formantValues[1,:]\n",
    "\n",
    "# TODO: Plot the recorded Formant inside the F1/F2 Vowel Chart (see Experiment 2)\n",
    "\n",
    "# Choose slider Export-Value which produces your desired plot\n",
    "exportRange = np.array([formant_tVec[0], formant_tVec[-1]])\n",
    "#exportRange = np.array([0 , 0.5]) # time range in sec\n",
    "\n",
    "plottitle = \"Recorded and detected Formants F1 & F2 in the Vowel Chart\"\n",
    "PM_timeWidget = SCplot.plot_F1_F2_in_vowel_chart(np.squeeze(snd.values), dt_snd, F1, F2, formant_tVec, plottitle,exportRange)\n",
    "widgets.HBox([PM_timeWidget])\n",
    "# TODO_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Authors of this Notebook\n",
    "Paul A. Bereuter, Clemens Frischmann, Florian Kraxberger\n",
    "\n",
    "*SPSC TU Graz, summer term 2020*  |  last modified: 15.09.2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
